"""
Utility functions for DANN training
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import json
import datasets.datasets as dataset
import models.models as models


def load_json(path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    with open(path, 'r') as f:
        return json.load(f)


def save_json(path, data):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    with open(path, 'w') as f:
        json.dump(data, f, indent=2)


def save_text(path, text):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    with open(path, 'w') as f:
        f.write(text)


def command_log(out_dir):
    """ã‚³ãƒãƒ³ãƒ‰ãƒ­ã‚°ã‚’ä¿å­˜"""
    import sys
    command = ' '.join(sys.argv)
    save_text(os.path.join(out_dir, 'command.txt'), command)


def parse_devices(device_str):
    """ãƒ‡ãƒã‚¤ã‚¹æ–‡å­—åˆ—ã‚’è§£æï¼ˆè¤‡æ•°GPUå®Œå…¨å¯¾å¿œï¼‰"""
    
    # CUDAåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
    if not torch.cuda.is_available():
        print("âŒ CUDA is not available!")
        return [0], torch.device('cpu')
    
    available_gpu_count = torch.cuda.device_count()
    print(f"ğŸ” Available GPUs: {available_gpu_count}")
    
    # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®è¤‡æ•°GPUæŒ‡å®š
    if ',' in device_str:
        device_parts = device_str.replace('cuda:', '').split(',')
        device_ids = []
        
        for part in device_parts:
            try:
                device_id = int(part.strip())
                if 0 <= device_id < available_gpu_count:
                    device_ids.append(device_id)
                else:
                    print(f"âš ï¸ GPU {device_id} is not available (max: {available_gpu_count-1})")
            except ValueError:
                print(f"âš ï¸ Invalid device specification: '{part}'")
        
        if not device_ids:
            print("âš ï¸ No valid GPUs found, using GPU 0")
            device_ids = [0]
        
        # é‡è¤‡é™¤å»ãƒ»ã‚½ãƒ¼ãƒˆ
        device_ids = sorted(list(set(device_ids)))
        primary_device = torch.device(f'cuda:{device_ids[0]}')
        
        print(f"âœ“ Using GPUs: {device_ids}, Primary: cuda:{device_ids[0]}")
        return device_ids, primary_device
    
    # å˜ä¸€GPUæŒ‡å®š
    else:
        device_str = device_str.replace('cuda:', '')
        try:
            device_id = int(device_str)
            if 0 <= device_id < available_gpu_count:
                primary_device = torch.device(f'cuda:{device_id}')
                print(f"âœ“ Using single GPU: cuda:{device_id}")
                return [device_id], primary_device
            else:
                print(f"âš ï¸ GPU {device_id} is not available, using GPU 0")
                return [0], torch.device('cuda:0')
        except ValueError:
            print(f"âš ï¸ Invalid device specification: '{device_str}', using GPU 0")
            return [0], torch.device('cuda:0')


def setup_cuda_environment(device_ids, seed=None):
    """CUDAç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆè¤‡æ•°GPUå¯¾å¿œï¼‰"""
    
    # è¤‡æ•°GPUç’°å¢ƒã®æœ€é©åŒ–
    if len(device_ids) > 1:
        print(f"ğŸš€ Setting up multi-GPU environment:")
        print(f"  CUDNN benchmark: True (for consistent input sizes)")
        print(f"  CUDNN deterministic: False (for performance)")
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
    else:
        torch.backends.cudnn.benchmark = True
    
    # ã‚·ãƒ¼ãƒ‰è¨­å®š
    if seed is not None:
        import random
        os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # å…¨GPUå¯¾å¿œ
        np.random.seed(seed)
        random.seed(seed)
        
        print(f"ğŸŒ± Random seed set: {seed} (applied to all {len(device_ids)} GPUs)")
        
        g = torch.Generator()
        g.manual_seed(seed)
        return g
    
    return None


def print_gpu_memory_info(device_ids):
    """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    print(f"ğŸ” GPU Memory Status:")
    
    for device_id in device_ids:
        try:
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if torch.cuda.is_available() and device_id < torch.cuda.device_count():
                torch.cuda.empty_cache()
                
                device_obj = torch.device(f'cuda:{device_id}')
                props = torch.cuda.get_device_properties(device_obj)
                total_mem = props.total_memory / 1e9
                allocated_mem = torch.cuda.memory_allocated(device_obj) / 1e9
                cached_mem = torch.cuda.memory_reserved(device_obj) / 1e9
                free_mem = total_mem - allocated_mem
                
                print(f"  GPU {device_id}: {props.name}")
                print(f"    Total: {total_mem:.2f} GB")
                print(f"    Allocated: {allocated_mem:.2f} GB ({allocated_mem/total_mem*100:.1f}%)")
                print(f"    Cached: {cached_mem:.2f} GB ({cached_mem/total_mem*100:.1f}%)")
                print(f"    Free: {free_mem:.2f} GB ({free_mem/total_mem*100:.1f}%)")
            else:
                print(f"  GPU {device_id}: Not available")
        except Exception as e:
            print(f"  GPU {device_id}: Error - {e}")


def get_datasets(config, fold, generator):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—"""
    # ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³
    loader_src, loader_eval_tr, loader_eval_vl = dataset.get_dataset(
        i_fold=fold, generator=generator, shuffle=True, **config['dataset']
    )
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³
    if 'dataset_target' in config:
        loader_target, _, _ = dataset.get_dataset(
            i_fold=fold, generator=generator, shuffle=True, **config['dataset_target']
        )
    else:
        loader_target = loader_src
    
    return loader_src, loader_eval_tr, loader_eval_vl, loader_target


def get_model_and_processors(config, device):
    """ãƒ¢ãƒ‡ãƒ«ã¨å‰å¾Œå‡¦ç†ã‚’å–å¾—"""
    backbone = models.get_model(**config['model']).to(device)
    pre, post, func, met = models.get_process(device=device, **config['process'])
    return backbone, pre, post, func, met


def init_optimizer(params, config):
    """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–"""
    opt_config = config['opt']
    name = opt_config['name']
    
    if name == 'sgd':
        kwargs = {k: v for k, v in opt_config.items() if k != 'name'}
        if 'momentum' in kwargs:
            kwargs['nesterov'] = True
        return optim.SGD(params, **kwargs)
    elif name == 'adam':
        return optim.Adam(params, **{k: v for k, v in opt_config.items() if k != 'name'})
    elif name == 'rmsprop':
        return optim.RMSprop(params, **{k: v for k, v in opt_config.items() if k != 'name'})
    else:
        raise ValueError(f"Unknown optimizer: {name}")


def get_scheduler(optimizer, config):
    """å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’å–å¾—"""
    if 'scheduler' not in config:
        return None
        
    scheduler_config = config['scheduler']
    if scheduler_config['name'] == 'lambda':
        gamma = scheduler_config.get('gamma', 0.1)
        decay_rate = scheduler_config.get('decay_rate', 0.001)
        return optim.lr_scheduler.LambdaLR(
            optimizer, 
            lambda x: gamma * (1. + decay_rate * float(x)) ** (-0.75)
        )
    return None


def is_integer_dtype(dtype):
    """dtype ãŒæ•´æ•°å‹ã‹ã©ã†ã‹ã‚’å®‰å…¨ã«åˆ¤å®š"""
    dtype_str = str(dtype)
    integer_types = ['int8', 'int16', 'int32', 'int64', 'uint8', 'bool']
    return any(int_type in dtype_str for int_type in integer_types)


def is_string_tensor(tensor):
    """ãƒ†ãƒ³ã‚½ãƒ«ãŒæ–‡å­—åˆ—å‹ã‹ã©ã†ã‹ã‚’å®‰å…¨ã«åˆ¤å®š"""
    if not isinstance(tensor, torch.Tensor):
        return False
    
    # PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ä¾å­˜ã—ãªã„æ–‡å­—åˆ—å‹ãƒã‚§ãƒƒã‚¯
    dtype_str = str(tensor.dtype)
    return 'object' in dtype_str


def convert_string_tensor_to_numeric(tensor, device):
    """æ–‡å­—åˆ—ãƒ†ãƒ³ã‚½ãƒ«ã‚’æ•°å€¤ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›"""
    try:
        if is_string_tensor(tensor):
            # æ–‡å­—åˆ—ãƒ†ãƒ³ã‚½ãƒ«ã®å ´åˆ
            if hasattr(tensor, 'tolist'):
                string_list = tensor.tolist()
            else:
                string_list = [item.item() if hasattr(item, 'item') else str(item) for item in tensor]
            
            # ä¸€æ„ãªãƒ©ãƒ™ãƒ«ã‚’å–å¾—ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
            unique_labels = sorted(list(set(string_list)))
            label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}
            indices = [label_to_idx[label] for label in string_list]
            return torch.tensor(indices, dtype=torch.long).to(device)
        else:
            # æ•°å€¤ãƒ†ãƒ³ã‚½ãƒ«ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™ï¼ˆå‹å¤‰æ›ã®ã¿ï¼‰
            if not is_integer_dtype(tensor.dtype):
                return tensor.long().to(device)
            else:
                return tensor.to(device)
    except Exception as e:
        print(f"âŒ String tensor conversion failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¼ãƒ­ãƒ†ãƒ³ã‚½ãƒ«
        return torch.zeros(len(tensor), dtype=torch.long).to(device)


def safe_batch_processing(batch, device, pre_function=None, is_evaluation=False):
    """ãƒãƒƒãƒã‚’å®‰å…¨ã«å‡¦ç†ï¼ˆä¿®æ­£ç‰ˆï¼šè©•ä¾¡æ™‚å‰å‡¦ç†å¼·åˆ¶é©ç”¨ï¼‰"""
    try:
        # è©•ä¾¡æ™‚ã‚‚å‰å‡¦ç†ã‚’å¿…ãšé©ç”¨
        if pre_function is not None:
            try:
                x, y = pre_function(batch, device, True)  # æœ€å¾Œã®å¼•æ•°ã‚’Trueã«å›ºå®š
                
                # ãƒ©ãƒ™ãƒ«å‡¦ç†
                if isinstance(y, dict):
                    if 'label' in y:
                        y = y['label']
                    elif 'ya' in y:
                        y = y['ya']
                    else:
                        y = list(y.values())[0]
                
                # å‹å¤‰æ›
                if isinstance(y, torch.Tensor):
                    if is_string_tensor(y):
                        y = convert_string_tensor_to_numeric(y, device)
                    elif not is_integer_dtype(y.dtype):
                        y = y.long().to(device)
                    else:
                        y = y.to(device)
                
                return x, y
                
            except Exception as e:
                print(f"Pre-processing failed: {e}, using manual processing")
        
        # æ‰‹å‹•å‡¦ç†ï¼ˆå‰å‡¦ç†ãŒãªã„å ´åˆã®ã¿ï¼‰
        if isinstance(batch, (list, tuple)) and len(batch) >= 2:
            x, y = batch[0], batch[1]
            
            # ãƒ‡ãƒ¼ã‚¿ã®å‹å¤‰æ›ã¨ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•
            if isinstance(x, torch.Tensor):
                x = x.to(device)
            elif isinstance(x, dict):
                x = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                     for k, v in x.items()}
            
            # ãƒ©ãƒ™ãƒ«å‡¦ç†
            if isinstance(y, torch.Tensor):
                if is_string_tensor(y):
                    y = convert_string_tensor_to_numeric(y, device)
                else:
                    y = y.long().to(device)
            elif isinstance(y, (list, tuple)):
                try:
                    y = torch.tensor(y, dtype=torch.long).to(device)
                except:
                    y = torch.zeros(len(y) if hasattr(y, '__len__') else 1, 
                                   dtype=torch.long).to(device)
            
            return x, y
        
        else:
            raise ValueError(f"Unexpected batch format: {type(batch)}")
        
    except Exception as e:
        print(f"âŒ Batch processing failed: {e}")
        import traceback
        traceback.print_exc()
        raise e


def safe_post_processing(y_pred, post_function, x, y):
    """å¾Œå‡¦ç†ã‚’å®‰å…¨ã«é©ç”¨"""
    if post_function is None:
        return y_pred
    
    try:
        import inspect
        sig = inspect.signature(post_function)
        params = list(sig.parameters.keys())
        
        if len(params) == 4:
            return post_function(y_pred, x, y, None)
        elif len(params) == 3:
            return post_function(y_pred, x, y)
        elif len(params) == 2:
            return post_function(y_pred, y)
        else:
            return post_function(y_pred)
            
    except Exception as e:
        print(f"Post-processing failed: {e}, using raw predictions")
        return y_pred


def set_alpha_safely(model, alpha):
    """DataParallelå¯¾å¿œã§set_alphaã‚’å®‰å…¨ã«å‘¼ã³å‡ºã—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    from torch.nn.parallel import DataParallel, DistributedDataParallel
    
    try:
        if isinstance(model, (DataParallel, DistributedDataParallel)):
            # .moduleã‚’é€šã—ã¦ã‚¢ã‚¯ã‚»ã‚¹
            if hasattr(model.module, 'set_alpha'):
                model.module.set_alpha(alpha)
            else:
                print(f"âš ï¸ Model module does not have set_alpha method")
        else:
            # ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
            if hasattr(model, 'set_alpha'):
                model.set_alpha(alpha)
            else:
                print(f"âš ï¸ Model does not have set_alpha method")
    except Exception as e:
        print(f"âš ï¸ Failed to set alpha: {e}")


def macro_sensitivity(y_pred, y_true, n_classes):
    """Macro Sensitivityè¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    try:
        from sklearn.metrics import confusion_matrix
        import numpy as np
        
        # äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        if y_pred.ndim > 1:
            y_pred_labels = np.argmax(y_pred, axis=1)
        else:
            y_pred_labels = y_pred
        
        # æ··åŒè¡Œåˆ—ã‚’ä½œæˆ
        cm = confusion_matrix(y_true, y_pred_labels, labels=range(n_classes))
        sensitivities = []
        
        for i in range(n_classes):
            tp = cm[i, i]  # True Positive
            fn = np.sum(cm[i, :]) - tp  # False Negative
            
            if tp + fn > 0:
                sensitivity = tp / (tp + fn)
                sensitivities.append(sensitivity)
            else:
                # ãã®ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
                sensitivities.append(0.0)
        
        # ãƒã‚¯ãƒ­å¹³å‡ã‚’è¨ˆç®—
        macro_sens = np.mean(sensitivities) if sensitivities else 0.0
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆæœ€åˆã®è©•ä¾¡æ™‚ã®ã¿ï¼‰
        if len(sensitivities) > 0:
            print(f"ğŸ“Š Class sensitivities: {[f'{s:.3f}' for s in sensitivities]}, Macro: {macro_sens:.3f}")
        
        return macro_sens
        
    except Exception as e:
        print(f"âŒ Macro sensitivity calculation failed: {e}")
        return 0.5  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯


class ForeverDataIterator:
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ç„¡é™ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿"""
    def __init__(self, data_loader):
        self.data_loader = data_loader
        self.iter = iter(self.data_loader)
    
    def __next__(self):
        try:
            return next(self.iter)
        except StopIteration:
            self.iter = iter(self.data_loader)
            return next(self.iter)
