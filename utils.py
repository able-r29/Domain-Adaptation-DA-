"""
Utility functions for DANN training
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import json
import datasets.datasets as dataset
import models.models as models


def load_json(path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    with open(path, 'r') as f:
        return json.load(f)


def save_json(path, data):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    with open(path, 'w') as f:
        json.dump(data, f, indent=2)


def save_text(path, text):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    with open(path, 'w') as f:
        f.write(text)


def command_log(out_dir):
    """ã‚³ãƒãƒ³ãƒ‰ãƒ­ã‚°ã‚’ä¿å­˜"""
    import sys
    command = ' '.join(sys.argv)
    save_text(os.path.join(out_dir, 'command.txt'), command)


def parse_devices(device_str):
    """ãƒ‡ãƒã‚¤ã‚¹æ–‡å­—åˆ—ã‚’è§£æ"""
    if ',' in device_str:
        device_parts = device_str.replace('cuda:', '').split(',')
        device_ids = []
        for part in device_parts:
            try:
                device_id = int(part.strip())
                if torch.cuda.is_available() and device_id < torch.cuda.device_count():
                    device_ids.append(device_id)
            except ValueError:
                pass
        
        if not device_ids:
            device_ids = [0]
            
        primary_device = torch.device(f'cuda:{device_ids[0]}')
        return device_ids, primary_device
    else:
        device_id = int(device_str.replace('cuda:', ''))
        return [device_id], torch.device(f'cuda:{device_id}')


def setup_cuda_environment(device_ids, seed=None):
    """CUDAç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    torch.backends.cudnn.benchmark = True
    
    if seed is not None:
        import random
        os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'
        torch.manual_seed(seed)
        np.random.seed(seed)
        random.seed(seed)
        torch.cuda.manual_seed_all(seed)
        g = torch.Generator()
        g.manual_seed(seed)
        return g
    return None


def print_gpu_memory_info(device_ids):
    """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º"""
    for device_id in device_ids:
        torch.cuda.empty_cache()
        print(f"ğŸ” GPU {device_id} memory:")
        device_obj = torch.device(f'cuda:{device_id}')
        total_mem = torch.cuda.get_device_properties(device_obj).total_memory / 1e9
        allocated_mem = torch.cuda.memory_allocated(device_obj) / 1e9
        print(f"  Total: {total_mem:.2f} GB, Allocated: {allocated_mem:.2f} GB, Free: {total_mem - allocated_mem:.2f} GB")


def get_datasets(config, fold, generator):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—"""
    # ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³
    loader_src, loader_eval_tr, loader_eval_vl = dataset.get_dataset(
        i_fold=fold, generator=generator, shuffle=True, **config['dataset']
    )
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³
    if 'dataset_target' in config:
        loader_target, _, _ = dataset.get_dataset(
            i_fold=fold, generator=generator, shuffle=True, **config['dataset_target']
        )
    else:
        loader_target = loader_src
    
    return loader_src, loader_eval_tr, loader_eval_vl, loader_target


def get_model_and_processors(config, device):
    """ãƒ¢ãƒ‡ãƒ«ã¨å‰å¾Œå‡¦ç†ã‚’å–å¾—"""
    backbone = models.get_model(**config['model']).to(device)
    pre, post, func, met = models.get_process(device=device, **config['process'])
    return backbone, pre, post, func, met


def init_optimizer(params, config):
    """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–"""
    opt_config = config['opt']
    name = opt_config['name']
    
    if name == 'sgd':
        kwargs = {k: v for k, v in opt_config.items() if k != 'name'}
        if 'momentum' in kwargs:
            kwargs['nesterov'] = True
        return optim.SGD(params, **kwargs)
    elif name == 'adam':
        return optim.Adam(params, **{k: v for k, v in opt_config.items() if k != 'name'})
    elif name == 'rmsprop':
        return optim.RMSprop(params, **{k: v for k, v in opt_config.items() if k != 'name'})
    else:
        raise ValueError(f"Unknown optimizer: {name}")


def get_scheduler(optimizer, config):
    """å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’å–å¾—"""
    if 'scheduler' not in config:
        return None
        
    scheduler_config = config['scheduler']
    if scheduler_config['name'] == 'lambda':
        gamma = scheduler_config.get('gamma', 0.1)
        decay_rate = scheduler_config.get('decay_rate', 0.001)
        return optim.lr_scheduler.LambdaLR(
            optimizer, 
            lambda x: gamma * (1. + decay_rate * float(x)) ** (-0.75)
        )
    return None


def is_integer_dtype(dtype):
    """dtype ãŒæ•´æ•°å‹ã‹ã©ã†ã‹ã‚’å®‰å…¨ã«åˆ¤å®š"""
    dtype_str = str(dtype)
    integer_types = ['int8', 'int16', 'int32', 'int64', 'uint8', 'bool']
    return any(int_type in dtype_str for int_type in integer_types)


def is_string_tensor(tensor):
    """ãƒ†ãƒ³ã‚½ãƒ«ãŒæ–‡å­—åˆ—å‹ã‹ã©ã†ã‹ã‚’å®‰å…¨ã«åˆ¤å®š"""
    if not isinstance(tensor, torch.Tensor):
        return False
    
    # PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ä¾å­˜ã—ãªã„æ–‡å­—åˆ—å‹ãƒã‚§ãƒƒã‚¯
    dtype_str = str(tensor.dtype)
    return 'object' in dtype_str


def convert_string_tensor_to_numeric(tensor, device):
    """æ–‡å­—åˆ—ãƒ†ãƒ³ã‚½ãƒ«ã‚’æ•°å€¤ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›"""
    try:
        if is_string_tensor(tensor):
            # æ–‡å­—åˆ—ãƒ†ãƒ³ã‚½ãƒ«ã®å ´åˆ
            if hasattr(tensor, 'tolist'):
                string_list = tensor.tolist()
            else:
                string_list = [item.item() if hasattr(item, 'item') else str(item) for item in tensor]
            
            # ä¸€æ„ãªãƒ©ãƒ™ãƒ«ã‚’å–å¾—ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
            unique_labels = sorted(list(set(string_list)))
            label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}
            indices = [label_to_idx[label] for label in string_list]
            return torch.tensor(indices, dtype=torch.long).to(device)
        else:
            # æ•°å€¤ãƒ†ãƒ³ã‚½ãƒ«ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™ï¼ˆå‹å¤‰æ›ã®ã¿ï¼‰
            if not is_integer_dtype(tensor.dtype):
                return tensor.long().to(device)
            else:
                return tensor.to(device)
    except Exception as e:
        print(f"âŒ String tensor conversion failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¼ãƒ­ãƒ†ãƒ³ã‚½ãƒ«
        return torch.zeros(len(tensor), dtype=torch.long).to(device)


def safe_batch_processing(batch, device, pre_function=None, is_evaluation=False):
    """ãƒãƒƒãƒã‚’å®‰å…¨ã«å‡¦ç†ï¼ˆä¿®æ­£ç‰ˆï¼šè©•ä¾¡æ™‚å‰å‡¦ç†å¼·åˆ¶é©ç”¨ï¼‰"""
    try:
        # è©•ä¾¡æ™‚ã‚‚å‰å‡¦ç†ã‚’å¿…ãšé©ç”¨
        if pre_function is not None:
            try:
                x, y = pre_function(batch, device, True)  # æœ€å¾Œã®å¼•æ•°ã‚’Trueã«å›ºå®š
                
                # ãƒ©ãƒ™ãƒ«å‡¦ç†
                if isinstance(y, dict):
                    if 'label' in y:
                        y = y['label']
                    elif 'ya' in y:
                        y = y['ya']
                    else:
                        y = list(y.values())[0]
                
                # å‹å¤‰æ›
                if isinstance(y, torch.Tensor):
                    if is_string_tensor(y):
                        y = convert_string_tensor_to_numeric(y, device)
                    elif not is_integer_dtype(y.dtype):
                        y = y.long().to(device)
                    else:
                        y = y.to(device)
                
                return x, y
                
            except Exception as e:
                print(f"Pre-processing failed: {e}, using manual processing")
        
        # æ‰‹å‹•å‡¦ç†ï¼ˆå‰å‡¦ç†ãŒãªã„å ´åˆã®ã¿ï¼‰
        if isinstance(batch, (list, tuple)) and len(batch) >= 2:
            x, y = batch[0], batch[1]
            
            # ãƒ‡ãƒ¼ã‚¿ã®å‹å¤‰æ›ã¨ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•
            if isinstance(x, torch.Tensor):
                x = x.to(device)
            elif isinstance(x, dict):
                x = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                     for k, v in x.items()}
            
            # ãƒ©ãƒ™ãƒ«å‡¦ç†
            if isinstance(y, torch.Tensor):
                if is_string_tensor(y):
                    y = convert_string_tensor_to_numeric(y, device)
                else:
                    y = y.long().to(device)
            elif isinstance(y, (list, tuple)):
                try:
                    y = torch.tensor(y, dtype=torch.long).to(device)
                except:
                    y = torch.zeros(len(y) if hasattr(y, '__len__') else 1, 
                                   dtype=torch.long).to(device)
            
            return x, y
        
        else:
            raise ValueError(f"Unexpected batch format: {type(batch)}")
        
    except Exception as e:
        print(f"âŒ Batch processing failed: {e}")
        import traceback
        traceback.print_exc()
        raise e


def safe_post_processing(y_pred, post_function, x, y):
    """å¾Œå‡¦ç†ã‚’å®‰å…¨ã«é©ç”¨"""
    if post_function is None:
        return y_pred
    
    try:
        import inspect
        sig = inspect.signature(post_function)
        params = list(sig.parameters.keys())
        
        if len(params) == 4:
            return post_function(y_pred, x, y, None)
        elif len(params) == 3:
            return post_function(y_pred, x, y)
        elif len(params) == 2:
            return post_function(y_pred, y)
        else:
            return post_function(y_pred)
            
    except Exception as e:
        print(f"Post-processing failed: {e}, using raw predictions")
        return y_pred


def set_alpha_safely(model, alpha):
    """DataParallelå¯¾å¿œã§set_alphaã‚’å®‰å…¨ã«å‘¼ã³å‡ºã—"""
    from torch.nn.parallel import DataParallel, DistributedDataParallel
    
    if isinstance(model, (DataParallel, DistributedDataParallel)):
        # .moduleã‚’é€šã—ã¦ã‚¢ã‚¯ã‚»ã‚¹
        model.module.set_alpha(alpha)
    else:
        # ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
        model.set_alpha(alpha)


def macro_sensitivity(y_pred, y_true, n_classes):
    """Macro Sensitivityè¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    try:
        from sklearn.metrics import confusion_matrix
        import numpy as np
        
        # äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        if y_pred.ndim > 1:
            y_pred_labels = np.argmax(y_pred, axis=1)
        else:
            y_pred_labels = y_pred
        
        # æ··åŒè¡Œåˆ—ã‚’ä½œæˆ
        cm = confusion_matrix(y_true, y_pred_labels, labels=range(n_classes))
        sensitivities = []
        
        for i in range(n_classes):
            tp = cm[i, i]  # True Positive
            fn = np.sum(cm[i, :]) - tp  # False Negative
            
            if tp + fn > 0:
                sensitivity = tp / (tp + fn)
                sensitivities.append(sensitivity)
            else:
                # ãã®ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
                sensitivities.append(0.0)
        
        # ãƒã‚¯ãƒ­å¹³å‡ã‚’è¨ˆç®—
        macro_sens = np.mean(sensitivities) if sensitivities else 0.0
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆæœ€åˆã®è©•ä¾¡æ™‚ã®ã¿ï¼‰
        if len(sensitivities) > 0:
            print(f"ğŸ“Š Class sensitivities: {[f'{s:.3f}' for s in sensitivities]}, Macro: {macro_sens:.3f}")
        
        return macro_sens
        
    except Exception as e:
        print(f"âŒ Macro sensitivity calculation failed: {e}")
        return 0.5  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯


class ForeverDataIterator:
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ç„¡é™ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿"""
    def __init__(self, data_loader):
        self.data_loader = data_loader
        self.iter = iter(self.data_loader)
    
    def __next__(self):
        try:
            return next(self.iter)
        except StopIteration:
            self.iter = iter(self.data_loader)
            return next(self.iter)
