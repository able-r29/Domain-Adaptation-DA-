#!/usr/bin/env python3
import os
import json
import argparse
from pathlib import Path
import shutil

class ImageCleanupTool:
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ä¸€è‡´ã—ãªã„ç”»åƒã‚’å‰Šé™¤ã™ã‚‹ãƒ„ãƒ¼ãƒ«"""
    
    def __init__(self, image_dir, json_file, backup_dir=None, dry_run=False):
        self.image_dir = Path(image_dir)
        self.json_file = Path(json_file)
        self.backup_dir = Path(backup_dir) if backup_dir else None
        self.dry_run = dry_run
        
        # ã‚µãƒãƒ¼ãƒˆã™ã‚‹ç”»åƒæ‹¡å¼µå­
        self.image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
        
        print(f"ğŸ“ ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.image_dir}")
        print(f"ğŸ“„ JSONãƒ•ã‚¡ã‚¤ãƒ«: {self.json_file}")
        print(f"ğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰: {'ON' if self.dry_run else 'OFF'}")
        if self.backup_dir:
            print(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.backup_dir}")
    
    def load_json_filenames(self):
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰filenameãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        print(f"\nğŸ“‹ JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {self.json_file}")
        
        try:
            with open(self.json_file, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            filenames = set()
            
            if isinstance(json_data, dict):
                # è¾æ›¸å½¢å¼ã®å ´åˆï¼ˆã‚­ãƒ¼ãŒãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
                filenames = set(json_data.keys())
                print(f"   å½¢å¼: è¾æ›¸ ({len(filenames)}å€‹ã®ã‚­ãƒ¼)")
                
            elif isinstance(json_data, list):
                # ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆ
                print(f"   å½¢å¼: ãƒªã‚¹ãƒˆ ({len(json_data)}å€‹ã®è¦ç´ )")
                
                for i, item in enumerate(json_data):
                    if isinstance(item, dict):
                        # è¤‡æ•°ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
                        for field in ['filename', 'original_filename', 'copied_filename']:
                            if field in item and item[field]:
                                # ãƒ‘ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ãƒ ã‚’å–å¾—
                                filename = os.path.basename(item[field])
                                filenames.add(filename)
                                break
                        else:
                            print(f"   è­¦å‘Š: ã‚¢ã‚¤ãƒ†ãƒ {i}ã«ãƒ•ã‚¡ã‚¤ãƒ«åãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    elif isinstance(item, str):
                        # æ–‡å­—åˆ—ã®å ´åˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ï¼‰
                        filename = os.path.basename(item)
                        filenames.add(filename)
            
            else:
                raise ValueError(f"æœªå¯¾å¿œã®JSONãƒ‡ãƒ¼ã‚¿å½¢å¼: {type(json_data)}")
            
            print(f"âœ… æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å: {len(filenames)}å€‹")
            
            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            sample_filenames = list(filenames)[:5]
            print(f"   ã‚µãƒ³ãƒ—ãƒ«: {sample_filenames}")
            
            return filenames
            
        except FileNotFoundError:
            print(f"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.json_file}")
            raise
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            raise
        except Exception as e:
            print(f"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def find_image_files(self):
        """ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        print(f"\nğŸ” ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢: {self.image_dir}")
        
        if not self.image_dir.exists():
            print(f"âŒ ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.image_dir}")
            raise FileNotFoundError(f"ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.image_dir}")
        
        image_files = []
        
        # å†å¸°çš„ã«ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        for file_path in self.image_dir.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in self.image_extensions:
                image_files.append(file_path)
        
        print(f"âœ… æ¤œå‡ºã—ãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«: {len(image_files)}å€‹")
        
        # æ‹¡å¼µå­åˆ¥ã®çµ±è¨ˆ
        ext_counts = {}
        for file_path in image_files:
            ext = file_path.suffix.lower()
            ext_counts[ext] = ext_counts.get(ext, 0) + 1
        
        print(f"   æ‹¡å¼µå­åˆ¥çµ±è¨ˆ: {dict(ext_counts)}")
        
        return image_files
    
    def identify_orphan_images(self, image_files, valid_filenames):
        """JSONã«å­˜åœ¨ã—ãªã„å­¤ç«‹ã—ãŸç”»åƒã‚’ç‰¹å®š"""
        print(f"\nğŸ” å­¤ç«‹ç”»åƒã®ç‰¹å®š")
        
        orphan_images = []
        matched_images = []
        
        for image_path in image_files:
            filename = image_path.name
            
            if filename in valid_filenames:
                matched_images.append(image_path)
            else:
                orphan_images.append(image_path)
        
        print(f"âœ… ä¸€è‡´ã—ãŸç”»åƒ: {len(matched_images)}å€‹")
        print(f"âš ï¸  å­¤ç«‹ã—ãŸç”»åƒ: {len(orphan_images)}å€‹")
        
        # å­¤ç«‹ç”»åƒã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        if orphan_images:
            print(f"\n   å­¤ç«‹ç”»åƒã‚µãƒ³ãƒ—ãƒ«:")
            for orphan in orphan_images[:10]:
                print(f"     {orphan}")
            if len(orphan_images) > 10:
                print(f"     ... ä»– {len(orphan_images) - 10}å€‹")
        
        return orphan_images, matched_images
    
    def create_backup_dir(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        if self.backup_dir and not self.dry_run:
            self.backup_dir.mkdir(parents=True, exist_ok=True)
            print(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {self.backup_dir}")
    
    def delete_orphan_images(self, orphan_images):
        """å­¤ç«‹ã—ãŸç”»åƒã‚’å‰Šé™¤ï¼ˆã¾ãŸã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰"""
        if not orphan_images:
            print(f"\nğŸ‰ å‰Šé™¤å¯¾è±¡ã®å­¤ç«‹ç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“")
            return
        
        print(f"\nğŸ—‘ï¸  å­¤ç«‹ç”»åƒã®å‰Šé™¤å‡¦ç†")
        print(f"   å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(orphan_images)}å€‹")
        
        if self.dry_run:
            print(f"   ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰: å®Ÿéš›ã®å‰Šé™¤ã¯è¡Œã„ã¾ã›ã‚“")
            
            # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã§ã®å‰Šé™¤äºˆå®šãƒªã‚¹ãƒˆè¡¨ç¤º
            print(f"\n   å‰Šé™¤äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«:")
            for i, orphan in enumerate(orphan_images):
                print(f"     {i+1:3d}. {orphan}")
                if i >= 20:  # æœ€åˆã®20å€‹ã¾ã§è¡¨ç¤º
                    print(f"     ... ä»– {len(orphan_images) - 20}å€‹")
                    break
            return
        
        # å®Ÿéš›ã®å‰Šé™¤å‡¦ç†
        deleted_count = 0
        backed_up_count = 0
        failed_count = 0
        
        for i, orphan_path in enumerate(orphan_images):
            try:
                if self.backup_dir:
                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
                    relative_path = orphan_path.relative_to(self.image_dir)
                    backup_path = self.backup_dir / relative_path
                    backup_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    shutil.move(str(orphan_path), str(backup_path))
                    backed_up_count += 1
                    
                    if (i + 1) % 100 == 0:
                        print(f"     é€²æ—: {i + 1}/{len(orphan_images)} å€‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†")
                else:
                    # ç›´æ¥å‰Šé™¤
                    orphan_path.unlink()
                    deleted_count += 1
                    
                    if (i + 1) % 100 == 0:
                        print(f"     é€²æ—: {i + 1}/{len(orphan_images)} å€‹å‰Šé™¤å®Œäº†")
                        
            except Exception as e:
                print(f"   âŒ ã‚¨ãƒ©ãƒ¼ ({orphan_path}): {e}")
                failed_count += 1
        
        # çµæœã‚µãƒãƒªãƒ¼
        print(f"\nğŸ“Š å‰Šé™¤å‡¦ç†çµæœ:")
        if self.backup_dir:
            print(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¸ˆã¿: {backed_up_count}å€‹")
        else:
            print(f"   å‰Šé™¤æ¸ˆã¿: {deleted_count}å€‹")
        
        if failed_count > 0:
            print(f"   å¤±æ•—: {failed_count}å€‹")
        
        print(f"âœ… å‡¦ç†å®Œäº†")
    
    def create_report(self, orphan_images, matched_images, output_file="cleanup_report.txt"):
        """å‡¦ç†çµæœã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
        report_path = Path(output_file)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("ç”»åƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"å‡¦ç†æ—¥æ™‚: {Path(__file__).stat().st_mtime}\n")
            f.write(f"ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.image_dir}\n")
            f.write(f"JSONãƒ•ã‚¡ã‚¤ãƒ«: {self.json_file}\n")
            f.write(f"ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰: {'ON' if self.dry_run else 'OFF'}\n")
            if self.backup_dir:
                f.write(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.backup_dir}\n")
            f.write(f"\n")
            
            f.write(f"å‡¦ç†çµæœ:\n")
            f.write(f"  ä¸€è‡´ã—ãŸç”»åƒ: {len(matched_images)}å€‹\n")
            f.write(f"  å­¤ç«‹ã—ãŸç”»åƒ: {len(orphan_images)}å€‹\n\n")
            
            if orphan_images:
                f.write("å­¤ç«‹ã—ãŸç”»åƒãƒªã‚¹ãƒˆ:\n")
                for i, orphan in enumerate(orphan_images):
                    f.write(f"  {i+1:4d}. {orphan}\n")
        
        print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ: {report_path}")
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œ"""
        print(f"{'='*60}")
        print(f"ç”»åƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œé–‹å§‹")
        print(f"{'='*60}")
        
        try:
            # 1. JSONã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿
            valid_filenames = self.load_json_filenames()
            
            # 2. ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            image_files = self.find_image_files()
            
            # 3. å­¤ç«‹ç”»åƒã‚’ç‰¹å®š
            orphan_images, matched_images = self.identify_orphan_images(image_files, valid_filenames)
            
            # 4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            if self.backup_dir:
                self.create_backup_dir()
            
            # 5. å­¤ç«‹ç”»åƒã‚’å‰Šé™¤
            self.delete_orphan_images(orphan_images)
            
            # 6. ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
            self.create_report(orphan_images, matched_images)
            
            print(f"\nğŸ‰ ç”»åƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†å®Œäº†!")
            
            return {
                'total_images': len(image_files),
                'matched_images': len(matched_images),
                'orphan_images': len(orphan_images)
            }
            
        except Exception as e:
            print(f"\nâŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            import traceback
            traceback.print_exc()
            raise

def main():
    parser = argparse.ArgumentParser(
        description="JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ä¸€è‡´ã—ãªã„å­¤ç«‹ã—ãŸç”»åƒã‚’å‰Šé™¤",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå®Ÿéš›ã®å‰Šé™¤ã¯è¡Œã‚ãªã„ï¼‰
  python image_cleanup.py --image_dir ./dataset_before --json_file ./dataset_before/train_metadata.json --dry-run

  # å‰Šé™¤å®Ÿè¡Œ
  python image_cleanup.py --image_dir ./dataset_before --json_file ./dataset_before/train_metadata.json

  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä»˜ãå‰Šé™¤
  python image_cleanup.py --image_dir ./dataset_before --json_file ./dataset_before/train_metadata.json --backup_dir ./backup

  # è¤‡æ•°JSONãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼ˆORæ¡ä»¶ï¼‰
  python image_cleanup.py --image_dir ./dataset_before --json_file ./dataset_before/train_metadata.json --json_file ./dataset_before/validation_metadata.json
        """
    )
    
    parser.add_argument('--image_dir', '-i', required=True, type=str,
                        help='ç”»åƒãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹')
    parser.add_argument('--json_file', '-j', action='append', required=True,
                        help='JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆè¤‡æ•°æŒ‡å®šå¯èƒ½ï¼‰')
    parser.add_argument('--backup_dir', '-b', type=str, default=None,
                        help='å‰Šé™¤ã™ã‚‹å‰ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--dry-run', '-d', action='store_true',
                        help='ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®å‰Šé™¤ã¯è¡Œã‚ãªã„ï¼‰')
    parser.add_argument('--report', '-r', type=str, default='cleanup_report.txt',
                        help='ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: cleanup_report.txtï¼‰')
    
    args = parser.parse_args()
    
    # è¤‡æ•°JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯çµ±åˆå‡¦ç†
    print("ğŸš€ ç”»åƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ„ãƒ¼ãƒ«")
    print(f"ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.image_dir}")
    print(f"JSONãƒ•ã‚¡ã‚¤ãƒ«: {args.json_file}")
    
    if len(args.json_file) == 1:
        # å˜ä¸€JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
        cleanup = ImageCleanupTool(
            image_dir=args.image_dir,
            json_file=args.json_file[0],
            backup_dir=args.backup_dir,
            dry_run=args.dry_run
        )
        result = cleanup.run()
        
    else:
        # è¤‡æ•°JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ - çµ±åˆå‡¦ç†
        print(f"ğŸ“‹ è¤‡æ•°JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆå‡¦ç†: {len(args.json_file)}å€‹")
        
        all_valid_filenames = set()
        
        # å…¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰filenameåé›†
        for json_file in args.json_file:
            print(f"\nğŸ“„ å‡¦ç†ä¸­: {json_file}")
            temp_cleanup = ImageCleanupTool(
                image_dir=args.image_dir,
                json_file=json_file,
                backup_dir=args.backup_dir,
                dry_run=True  # çµ±åˆæ™‚ã¯ä¸€æ™‚çš„ã«dry-run
            )
            filenames = temp_cleanup.load_json_filenames()
            all_valid_filenames.update(filenames)
        
        print(f"\nğŸ“Š çµ±åˆçµæœ: {len(all_valid_filenames)}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å")
        
        # çµ±åˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆã§å®Ÿéš›ã®å‡¦ç†
        # ä¸€æ™‚çš„ãªJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦å‡¦ç†
        temp_json = Path("temp_merged_filenames.json")
        with open(temp_json, 'w', encoding='utf-8') as f:
            json.dump(list(all_valid_filenames), f, indent=2)
        
        try:
            cleanup = ImageCleanupTool(
                image_dir=args.image_dir,
                json_file=temp_json,
                backup_dir=args.backup_dir,
                dry_run=args.dry_run
            )
            result = cleanup.run()
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            if temp_json.exists():
                temp_json.unlink()
    
    print(f"\nğŸ“Š æœ€çµ‚çµæœ:")
    print(f"  ç·ç”»åƒæ•°: {result['total_images']}")
    print(f"  ä¸€è‡´ç”»åƒæ•°: {result['matched_images']}")
    print(f"  å‰Šé™¤å¯¾è±¡ç”»åƒæ•°: {result['orphan_images']}")

if __name__ == "__main__":
    main()