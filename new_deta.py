import os
import json
import shutil
import random
from pathlib import Path
from collections import defaultdict, Counter
import numpy as np

class JSONBasedStratifiedDatasetCreator:
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹å±¤åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, json_path, image_dir, output_path, train_ratio=0.6, val_ratio=0.1, test1_ratio=0.15, test2_ratio=0.15, age_threshold=30):
        self.json_path = Path(json_path)
        self.image_dir = Path(image_dir)
        self.output_path = Path(output_path)
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test1_ratio = test1_ratio
        self.test2_ratio = test2_ratio
        self.age_threshold = age_threshold
        
        # æ¯”ç‡ã®åˆè¨ˆãŒ1.0ã«ãªã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        total_ratio = train_ratio + val_ratio + test1_ratio + test2_ratio
        if abs(total_ratio - 1.0) > 1e-6:
            raise ValueError(f"æ¯”ç‡ã®åˆè¨ˆãŒ1.0ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {total_ratio}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ JSONãƒ•ã‚¡ã‚¤ãƒ«: {self.json_path}")
        print(f"ğŸ“ ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.image_dir}")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_path}")
        print(f"ğŸ¯ å¹´é½¢é–¾å€¤: {self.age_threshold}æ­³")
        print(f"ğŸ“Š åˆ†å‰²æ¯”ç‡: Train={train_ratio:.1f} Val={val_ratio:.1f} Test1={test1_ratio:.1f} Test2={test2_ratio:.1f}")
    
    def load_json_data(self):
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print(f"\nğŸ“‹ JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {self.json_path}")
        
        try:
            with open(self.json_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            print(f"âœ… JSONãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {len(json_data)}ä»¶")
            return json_data
            
        except FileNotFoundError:
            print(f"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.json_path}")
            raise
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            raise
        except Exception as e:
            print(f"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def normalize_class_label(self, label_data):
        """ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’æ­£è¦åŒ–ï¼ˆ0: è‰¯æ€§, 1: æ‚ªæ€§ï¼‰"""
        if isinstance(label_data, list):
            label = label_data[0] if label_data else None
        else:
            label = label_data
        
        # æ§˜ã€…ãªå½¢å¼ã®ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’0/1ã«å¤‰æ›
        if label in [0, '0', 'benign', 'nevus', 'naevus']:
            return 0  # è‰¯æ€§ï¼ˆæ¯æ–‘ï¼‰
        elif label in [1, '1', 31, '31', 'malignant', 'melanoma']:
            return 1  # æ‚ªæ€§ï¼ˆãƒ¡ãƒ©ãƒãƒ¼ãƒï¼‰
        else:
            print(f"âš ï¸ æœªçŸ¥ã®ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«: {label}")
            return None
    
    def normalize_age(self, age_data):
        """å¹´é½¢ã‚’æ­£è¦åŒ–"""
        if isinstance(age_data, list):
            age = age_data[0] if age_data else None
        else:
            age = age_data
        
        try:
            age_num = int(float(age)) if age is not None else None
            return age_num
        except (ValueError, TypeError):
            print(f"âš ï¸ å¹´é½¢å¤‰æ›ã‚¨ãƒ©ãƒ¼: {age}")
            return None
    
    def normalize_body_part(self, part_data):
        """ç–¾æ‚£éƒ¨ä½ã‚’æ­£è¦åŒ–"""
        if isinstance(part_data, list):
            part = part_data[0] if part_data else None
        else:
            part = part_data
        
        if not part:
            return 'Unknown'
        
        # éƒ¨ä½åã®æ­£è¦åŒ–
        part_str = str(part).lower().strip()
        
        # éƒ¨ä½ãƒãƒƒãƒ”ãƒ³ã‚°
        part_mapping = {
            'leg': 'Leg',
            'legs': 'Leg',
            'lower_limb': 'Leg',
            'lower limb': 'Leg',
            'trunk': 'Trunk',
            'torso': 'Trunk',
            'body': 'Trunk',
            'chest': 'Trunk',
            'back': 'Trunk',
            'upperarm': 'Upper_Arm',
            'upper_arm': 'Upper_Arm',
            'upper arm': 'Upper_Arm',
            'arm': 'Upper_Arm',
            'arms': 'Upper_Arm',
            'upper_limb': 'Upper_Arm',
            'upper limb': 'Upper_Arm'
        }
        
        normalized_part = part_mapping.get(part_str, part)
        return normalized_part
    
    def get_facility_id(self, item):
        """æ–½è¨­IDã‚’å–å¾—"""
        # è¤‡æ•°ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰æ–½è¨­IDã‚’å–å¾—
        for key in ['univ_ID', 'facility', 'institution', 'site']:
            if key in item:
                facility = item[key]
                if isinstance(facility, list):
                    return facility[0] if facility else 'Unknown'
                return str(facility) if facility else 'Unknown'
        return 'Unknown'
    
    def process_json_data(self, json_data):
        """JSONãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã‚’ä½œæˆ"""
        print(f"\nğŸ”„ JSONãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")
        
        processed_data = []
        skipped_count = 0
        missing_files = []
        
        for i, item in enumerate(json_data):
            # ãƒ•ã‚¡ã‚¤ãƒ«åå–å¾—
            filename = item.get('filename')
            if not filename:
                # filenameãŒãªã„å ´åˆã€jpg_srcã‹ã‚‰å–å¾—ã‚’è©¦ã™
                jpg_src = item.get('jpg_src', '')
                if jpg_src:
                    filename = os.path.basename(jpg_src)
                else:
                    print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: item {i}")
                    skipped_count += 1
                    continue
            
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            image_path = self.image_dir / filename
            if not image_path.exists():
                missing_files.append(filename)
                continue
            
            # ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«æ­£è¦åŒ–
            class_label = None
            for key in ['LABEL', 'class', 'label', 'class_label']:
                if key in item:
                    class_label = self.normalize_class_label(item[key])
                    break
            
            if class_label is None:
                print(f"âš ï¸ ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filename}")
                skipped_count += 1
                continue
            
            # å¹´é½¢æ­£è¦åŒ–
            age = self.normalize_age(item.get('age'))
            if age is None:
                print(f"âš ï¸ å¹´é½¢æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filename}")
                skipped_count += 1
                continue
            
            # å¹´é½¢å±¤åˆ†é¡
            age_group = 'over_30' if age >= self.age_threshold else 'under_30'
            
            # ç–¾æ‚£éƒ¨ä½æ­£è¦åŒ–
            body_part = self.normalize_body_part(item.get('part'))
            if body_part == 'Unknown':
                print(f"âš ï¸ ç–¾æ‚£éƒ¨ä½ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filename}")
                skipped_count += 1
                continue
            
            # æ–½è¨­IDå–å¾—
            facility = self.get_facility_id(item)
            
            # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
            processed_item = {
                'filename': filename,
                'image_path': str(image_path),
                'class_label': class_label,
                'age': age,
                'age_group': age_group,
                'body_part': body_part,
                'facility': facility,
                'original_item': item  # å…ƒã®JSONã‚¢ã‚¤ãƒ†ãƒ ã‚’ä¿æŒ
            }
            processed_data.append(processed_item)
            
            # é€²æ—è¡¨ç¤º
            if (i + 1) % 1000 == 0:
                print(f"   å‡¦ç†æ¸ˆã¿: {i + 1}/{len(json_data)} ä»¶")
        
        print(f"\nğŸ“Š JSONå‡¦ç†çµæœ:")
        print(f"   ç·ã‚¢ã‚¤ãƒ†ãƒ æ•°: {len(json_data)}")
        print(f"   å‡¦ç†æˆåŠŸ: {len(processed_data)}")
        print(f"   ã‚¹ã‚­ãƒƒãƒ—: {skipped_count}")
        print(f"   ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ¬ æ: {len(missing_files)}")
        
        if missing_files:
            # æ¬ æãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä¿å­˜
            missing_log = self.output_path / "missing_files.txt"
            with open(missing_log, 'w', encoding='utf-8') as f:
                for missing_file in missing_files:
                    f.write(f"{missing_file}\n")
            print(f"   ğŸ“„ æ¬ æãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚°: {missing_log}")
        
        return processed_data
    
    def analyze_data_distribution(self, processed_data):
        """ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’åˆ†æ"""
        print(f"\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒåˆ†æ")
        
        # å„ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é›†è¨ˆ
        class_counts = Counter()
        age_group_counts = Counter()
        body_part_counts = Counter()
        facility_counts = Counter()
        combination_counts = Counter()
        
        for item in processed_data:
            class_counts[item['class_label']] += 1
            age_group_counts[item['age_group']] += 1
            body_part_counts[item['body_part']] += 1
            facility_counts[item['facility']] += 1
            
            # 3è¦ç´ ã®çµ„ã¿åˆã‚ã›
            combo = (item['class_label'], item['age_group'], item['body_part'])
            combination_counts[combo] += 1
        
        # çµæœè¡¨ç¤º
        print(f"\nã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:")
        for class_label, count in sorted(class_counts.items()):
            class_name = 'è‰¯æ€§ï¼ˆæ¯æ–‘ï¼‰' if class_label == 0 else 'æ‚ªæ€§ï¼ˆãƒ¡ãƒ©ãƒãƒ¼ãƒï¼‰'
            percentage = (count / len(processed_data)) * 100
            print(f"   Class {class_label} ({class_name}): {count}ä»¶ ({percentage:.1f}%)")
        
        print(f"\nå¹´é½¢å±¤åˆ†å¸ƒ:")
        for age_group, count in sorted(age_group_counts.items()):
            age_desc = f'{self.age_threshold}æ­³æœªæº€' if age_group == 'under_30' else f'{self.age_threshold}æ­³ä»¥ä¸Š'
            percentage = (count / len(processed_data)) * 100
            print(f"   {age_group} ({age_desc}): {count}ä»¶ ({percentage:.1f}%)")
        
        print(f"\nç–¾æ‚£éƒ¨ä½åˆ†å¸ƒ:")
        for body_part, count in sorted(body_part_counts.items()):
            percentage = (count / len(processed_data)) * 100
            print(f"   {body_part}: {count}ä»¶ ({percentage:.1f}%)")
        
        print(f"\næ–½è¨­åˆ†å¸ƒ:")
        for facility, count in sorted(facility_counts.items()):
            percentage = (count / len(processed_data)) * 100
            print(f"   {facility}: {count}ä»¶ ({percentage:.1f}%)")
        
        print(f"\nã‚¯ãƒ©ã‚¹Ã—å¹´é½¢å±¤Ã—ç–¾æ‚£éƒ¨ä½ã®çµ„ã¿åˆã‚ã›:")
        for combo, count in sorted(combination_counts.items()):
            class_label, age_group, body_part = combo
            class_name = 'è‰¯æ€§' if class_label == 0 else 'æ‚ªæ€§'
            age_desc = f'{self.age_threshold}æ­³æœªæº€' if age_group == 'under_30' else f'{self.age_threshold}æ­³ä»¥ä¸Š'
            print(f"   {class_name} Ã— {age_desc} Ã— {body_part}: {count}ä»¶")
        
        return {
            'class_counts': class_counts,
            'age_group_counts': age_group_counts,
            'body_part_counts': body_part_counts,
            'facility_counts': facility_counts,
            'combination_counts': combination_counts
        }
    
    def stratified_split_4way(self, processed_data):
        """ã‚¯ãƒ©ã‚¹ãƒ»å¹´é½¢å±¤ãƒ»ç–¾æ‚£éƒ¨ä½ã‚’è€ƒæ…®ã—ãŸ4åˆ†å‰²å±¤åŒ–åˆ†å‰²"""
        print(f"\nğŸ“‚ 4åˆ†å‰²å±¤åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²")
        print(f"ğŸ¯ Train:{self.train_ratio:.1f} Val:{self.val_ratio:.1f} Test1:{self.test1_ratio:.1f} Test2:{self.test2_ratio:.1f}")
        print("ğŸ¯ å„åˆ†å‰²ã§ã‚¯ãƒ©ã‚¹ãƒ»å¹´é½¢å±¤ãƒ»ç–¾æ‚£éƒ¨ä½ã®å‰²åˆã‚’ä¿æŒ")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        grouped_data = defaultdict(list)
        for data in processed_data:
            key = (data['class_label'], data['age_group'], data['body_part'])
            grouped_data[key].append(data)
        
        print(f"\nğŸ“Š çµ„ã¿åˆã‚ã›åˆ¥ãƒ‡ãƒ¼ã‚¿æ•°:")
        for key, data_list in grouped_data.items():
            class_label, age_group, body_part = key
            class_name = 'è‰¯æ€§' if class_label == 0 else 'æ‚ªæ€§'
            age_desc = f'{self.age_threshold}æ­³æœªæº€' if age_group == 'under_30' else f'{self.age_threshold}æ­³ä»¥ä¸Š'
            print(f"   {class_name} Ã— {age_desc} Ã— {body_part}: {len(data_list)}ä»¶")
        
        # å„çµ„ã¿åˆã‚ã›ã‚’4åˆ†å‰²
        splits = {
            'train': [],
            'validation': [], 
            'test1': [],
            'test2': []
        }
        
        split_counts = {
            'train': {'class': Counter(), 'age_group': Counter(), 'bodypart': Counter()},
            'validation': {'class': Counter(), 'age_group': Counter(), 'bodypart': Counter()},
            'test1': {'class': Counter(), 'age_group': Counter(), 'bodypart': Counter()},
            'test2': {'class': Counter(), 'age_group': Counter(), 'bodypart': Counter()}
        }
        
        for key, data_list in grouped_data.items():
            if not data_list:
                continue
            
            class_label, age_group, body_part = key
            
            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            random.seed(42)
            shuffled_data = data_list.copy()
            random.shuffle(shuffled_data)
            
            # åˆ†å‰²ç‚¹è¨ˆç®—
            total_count = len(shuffled_data)
            train_count = int(total_count * self.train_ratio)
            val_count = int(total_count * self.val_ratio)
            
            # æ®‹ã‚Šã‚’test1ã¨test2ã§åˆ†å‰²ï¼ˆå‡ç­‰åŒ–ï¼‰
            remaining_count = total_count - train_count - val_count
            test1_count = remaining_count // 2
            test2_count = remaining_count - test1_count
            
            # åˆ†å‰²å®Ÿè¡Œ
            train_data = shuffled_data[:train_count]
            val_data = shuffled_data[train_count:train_count + val_count]
            test1_data = shuffled_data[train_count + val_count:train_count + val_count + test1_count]
            test2_data = shuffled_data[train_count + val_count + test1_count:]
            
            # çµæœã«è¿½åŠ 
            for data in train_data:
                splits['train'].append(data)
                split_counts['train']['class'][data['class_label']] += 1
                split_counts['train']['age_group'][data['age_group']] += 1
                split_counts['train']['bodypart'][data['body_part']] += 1
            
            for data in val_data:
                splits['validation'].append(data)
                split_counts['validation']['class'][data['class_label']] += 1
                split_counts['validation']['age_group'][data['age_group']] += 1
                split_counts['validation']['bodypart'][data['body_part']] += 1
            
            for data in test1_data:
                splits['test1'].append(data)
                split_counts['test1']['class'][data['class_label']] += 1
                split_counts['test1']['age_group'][data['age_group']] += 1
                split_counts['test1']['bodypart'][data['body_part']] += 1
            
            for data in test2_data:
                splits['test2'].append(data)
                split_counts['test2']['class'][data['class_label']] += 1
                split_counts['test2']['age_group'][data['age_group']] += 1
                split_counts['test2']['bodypart'][data['body_part']] += 1
            
            class_name = 'è‰¯æ€§' if class_label == 0 else 'æ‚ªæ€§'
            age_desc = f'{self.age_threshold}æ­³æœªæº€' if age_group == 'under_30' else f'{self.age_threshold}æ­³ä»¥ä¸Š'
            print(f"   {class_name}-{age_desc}-{body_part}: Train={len(train_data)} Val={len(val_data)} Test1={len(test1_data)} Test2={len(test2_data)}")
        
        # test1ã¨test2ã®æšæ•°å·®ã‚’ç¢ºèª
        test1_total = len(splits['test1'])
        test2_total = len(splits['test2'])
        test_diff = abs(test1_total - test2_total)
        
        print(f"\nğŸ¯ Teståˆ†å‰²å‡ç­‰åŒ–çµæœ:")
        print(f"   Test1: {test1_total}ä»¶")
        print(f"   Test2: {test2_total}ä»¶")
        print(f"   å·®åˆ†: {test_diff}ä»¶")
        
        # å„åˆ†å‰²ã®çµ±è¨ˆè¡¨ç¤º
        for split_name in ['train', 'validation', 'test1', 'test2']:
            total_images = len(splits[split_name])
            print(f"\nğŸ“Š {split_name.upper()}: {total_images}ä»¶")
            
            # ã‚¯ãƒ©ã‚¹åˆ¥çµ±è¨ˆ
            print(f"    ã‚¯ãƒ©ã‚¹åˆ¥:")
            for class_label, count in sorted(split_counts[split_name]['class'].items()):
                percentage = (count / total_images) * 100 if total_images > 0 else 0
                class_name = 'è‰¯æ€§' if class_label == 0 else 'æ‚ªæ€§'
                print(f"      Class {class_label} ({class_name}): {count}ä»¶ ({percentage:.1f}%)")
            
            # å¹´é½¢å±¤åˆ¥çµ±è¨ˆ
            print(f"    å¹´é½¢å±¤åˆ¥:")
            for age_group, count in sorted(split_counts[split_name]['age_group'].items()):
                percentage = (count / total_images) * 100 if total_images > 0 else 0
                age_desc = f'{self.age_threshold}æ­³æœªæº€' if age_group == 'under_30' else f'{self.age_threshold}æ­³ä»¥ä¸Š'
                print(f"      {age_group} ({age_desc}): {count}ä»¶ ({percentage:.1f}%)")
            
            # ç–¾æ‚£éƒ¨ä½åˆ¥çµ±è¨ˆ
            print(f"    ç–¾æ‚£éƒ¨ä½åˆ¥:")
            for body_part, count in sorted(split_counts[split_name]['bodypart'].items()):
                percentage = (count / total_images) * 100 if total_images > 0 else 0
                print(f"      {body_part}: {count}ä»¶ ({percentage:.1f}%)")
        
        return splits, split_counts
    
    def copy_images_to_splits(self, splits):
        """å„åˆ†å‰²ã«ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼"""
        print(f"\nğŸ“ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒ”ãƒ¼é–‹å§‹")
        
        copy_summary = {
            'train': 0,
            'validation': 0,
            'test1': 0,
            'test2': 0
        }
        
        for split_name, data_list in splits.items():
            print(f"\nğŸ”„ {split_name.upper()} åˆ†å‰²ã¸ã®ç”»åƒã‚³ãƒ”ãƒ¼: {len(data_list)}ä»¶")
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            split_dir = self.output_path / split_name
            split_dir.mkdir(parents=True, exist_ok=True)
            
            successful_copies = 0
            failed_copies = []
            
            for i, data in enumerate(data_list):
                source_path = Path(data['image_path'])
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if not source_path.exists():
                    print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {source_path}")
                    failed_copies.append(str(source_path))
                    continue
                
                # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®šï¼ˆé‡è¤‡å›é¿ï¼‰
                dest_path = split_dir / data['filename']
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åé‡è¤‡ã®å ´åˆã¯é€£ç•ªã‚’è¿½åŠ 
                counter = 1
                while dest_path.exists():
                    stem = Path(data['filename']).stem
                    suffix = Path(data['filename']).suffix
                    new_name = f"{stem}_{counter:03d}{suffix}"
                    dest_path = split_dir / new_name
                    data['copied_filename'] = new_name  # ã‚³ãƒ”ãƒ¼å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨˜éŒ²
                    counter += 1
                
                if 'copied_filename' not in data:
                    data['copied_filename'] = data['filename']
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼å®Ÿè¡Œ
                try:
                    shutil.copy2(source_path, dest_path)
                    successful_copies += 1
                    
                    # é€²æ—è¡¨ç¤ºï¼ˆ100æšã”ã¨ï¼‰
                    if (i + 1) % 100 == 0:
                        print(f"   é€²æ—: {i + 1}/{len(data_list)} æšå®Œäº†")
                        
                except Exception as e:
                    print(f"âŒ ã‚³ãƒ”ãƒ¼ã‚¨ãƒ©ãƒ¼: {source_path} â†’ {dest_path}: {e}")
                    failed_copies.append(f"{source_path} (ã‚¨ãƒ©ãƒ¼: {e})")
            
            copy_summary[split_name] = successful_copies
            
            print(f"   âœ… {split_name.upper()}: {successful_copies}/{len(data_list)} æšã‚³ãƒ”ãƒ¼å®Œäº†")
            
            if failed_copies:
                print(f"   âš ï¸ å¤±æ•—: {len(failed_copies)} æš")
                # å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’ä¿å­˜
                failed_log_path = self.output_path / f"{split_name}_failed_copies.txt"
                with open(failed_log_path, 'w', encoding='utf-8') as f:
                    for failed_file in failed_copies:
                        f.write(f"{failed_file}\n")
                print(f"   ğŸ“„ å¤±æ•—ãƒ­ã‚°ä¿å­˜: {failed_log_path}")
        
        return copy_summary
    
    def create_metadata_files(self, splits):
        """å„åˆ†å‰²ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        print(f"\nğŸ“„ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ")
        
        for split_name, data_list in splits.items():
            metadata_list = []
            
            for data in data_list:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                metadata_item = {
                    'filename': data.get('copied_filename', data['filename']),
                    'original_filename': data['filename'],
                    'class_label': data['class_label'],
                    'age': data['age'],
                    'age_group': data['age_group'],
                    'body_part': data['body_part'],
                    'facility': data['facility'],
                    'image_path': data['image_path'],
                    'LABEL': data['class_label'],  # æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨ã®äº’æ›æ€§
                    'part': [data['body_part']],   # ãƒªã‚¹ãƒˆå½¢å¼ã§ã®äº’æ›æ€§
                    'age': [data['age']],          # ãƒªã‚¹ãƒˆå½¢å¼ã§ã®äº’æ›æ€§
                    'univ_ID': data['facility']    # æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨ã®äº’æ›æ€§
                }
                
                # å…ƒã®JSONã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰ä»–ã®æƒ…å ±ã‚‚ã‚³ãƒ”ãƒ¼
                original_item = data.get('original_item', {})
                for key, value in original_item.items():
                    if key not in metadata_item:
                        metadata_item[key] = value
                
                metadata_list.append(metadata_item)
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            metadata_file = self.output_path / f"{split_name}_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata_list, f, indent=2, ensure_ascii=False)
            
            print(f"   âœ… {split_name}_metadata.json: {len(metadata_list)} ä»¶")
        
        print(f"ğŸ“„ å…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†")
    
    def create_dataset_info(self, split_counts, data_stats, copy_summary):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’è©³ç´°ã«è¨˜éŒ²"""
        # data_statsã®combination_countsã®ã‚¿ãƒ—ãƒ«ã‚­ãƒ¼ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        processed_data_stats = {}
        for key, value in data_stats.items():
            if key == 'combination_counts':
                # ã‚¿ãƒ—ãƒ«ã‚­ãƒ¼ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                string_keyed_combinations = {}
                for combo_tuple, count in value.items():
                    class_label, age_group, body_part = combo_tuple
                    class_name = 'è‰¯æ€§' if class_label == 0 else 'æ‚ªæ€§'
                    combo_str = f"{class_name}_{age_group}_{body_part}"
                    string_keyed_combinations[combo_str] = count
                processed_data_stats[key] = string_keyed_combinations
            else:
                # Counter ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸ã«å¤‰æ›
                if hasattr(value, 'items'):
                    processed_data_stats[key] = dict(value)
                else:
                    processed_data_stats[key] = value
        
        dataset_info = {
            'source_description': f'JSONãƒ™ãƒ¼ã‚¹å±¤åŒ–åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (å¹´é½¢é–¾å€¤: {self.age_threshold}æ­³)',
            'source_json': str(self.json_path),
            'source_image_dir': str(self.image_dir),
            'age_threshold': self.age_threshold,
            'classes': [0, 1],
            'class_names': {0: 'è‰¯æ€§ï¼ˆæ¯æ–‘ï¼‰', 1: 'æ‚ªæ€§ï¼ˆãƒ¡ãƒ©ãƒãƒ¼ãƒï¼‰'},
            'age_groups': ['under_30', 'over_30'],
            'split_ratios': {
                'train': self.train_ratio,
                'validation': self.val_ratio,
                'test1': self.test1_ratio,
                'test2': self.test2_ratio
            },
            'data_statistics': processed_data_stats,
            'split_statistics': {},
            'copy_summary': copy_summary
        }
        
        # å„åˆ†å‰²ã®çµ±è¨ˆæƒ…å ±
        for split_name in ['train', 'validation', 'test1', 'test2']:
            dataset_info['split_statistics'][split_name] = {
                'class_counts': dict(split_counts[split_name]['class']),
                'age_group_counts': dict(split_counts[split_name]['age_group']),
                'bodypart_counts': dict(split_counts[split_name]['bodypart']),
                'total_images': sum(split_counts[split_name]['class'].values())
            }
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        info_file = self.output_path / "dataset_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ä¿å­˜: {info_file}")
        return dataset_info
    
    def create_dataset(self):
        """JSONãƒ™ãƒ¼ã‚¹å±¤åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ JSONãƒ™ãƒ¼ã‚¹ 4åˆ†å‰²å±¤åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
        print(f"ğŸ¯ Train:{self.train_ratio:.1f} Val:{self.val_ratio:.1f} Test1:{self.test1_ratio:.1f} Test2:{self.test2_ratio:.1f}")
        print(f"ğŸ¯ ã‚¯ãƒ©ã‚¹ãƒ»å¹´é½¢å±¤ãƒ»ç–¾æ‚£éƒ¨ä½å‡ç­‰åˆ†å‰²")
        print(f"{'='*80}")
        
        # 1. JSONãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        json_data = self.load_json_data()
        
        # 2. JSONãƒ‡ãƒ¼ã‚¿å‡¦ç†
        processed_data = self.process_json_data(json_data)
        
        if not processed_data:
            print(f"âŒ å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        # 3. ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒåˆ†æ
        data_stats = self.analyze_data_distribution(processed_data)
        
        # 4. 4åˆ†å‰²å±¤åŒ–åˆ†å‰²
        splits, split_counts = self.stratified_split_4way(processed_data)
        
        # 5. ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
        copy_summary = self.copy_images_to_splits(splits)
        
        # 6. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        self.create_metadata_files(splits)
        
        # 7. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ä¿å­˜
        dataset_info = self.create_dataset_info(split_counts, data_stats, copy_summary)
        
        print(f"\nğŸ‰ JSONãƒ™ãƒ¼ã‚¹å±¤åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†!")
        print(f"ğŸ“ å‡ºåŠ›å…ˆ: {self.output_path}")
        
        return {
            'splits': splits,
            'split_counts': split_counts,
            'copy_summary': copy_summary,
            'dataset_info': dataset_info,
            'data_stats': data_stats
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=== JSONãƒ™ãƒ¼ã‚¹ 4åˆ†å‰²å±¤åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆãƒ„ãƒ¼ãƒ« ===")
    
    # è¨­å®š
    json_path = "./YN_Under_30_Body_dataset_fixed.json"  # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    image_dir = "./dataset_merged_body/YN/Under_30/Body"              # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_path = "./dataset_before_YN_U30body"
    train_ratio = 0.6
    val_ratio = 0.2
    test1_ratio = 0.1
    test2_ratio = 0.1
    age_threshold = 30  # å¹´é½¢ã®é–¾å€¤
    
    print(f"ğŸ“‹ å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«: {json_path}")
    print(f"ğŸ“ ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {image_dir}")
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_path}")
    print(f"ğŸ¯ å¹´é½¢é–¾å€¤: {age_threshold}æ­³")
    print(f"ğŸ“Š åˆ†å‰²æ¯”ç‡: Train={train_ratio:.1f} Val={val_ratio:.1f} Test1={test1_ratio:.1f} Test2={test2_ratio:.1f}")
    print(f"ğŸ¯ å±¤åŒ–åˆ†å‰²: ã‚¯ãƒ©ã‚¹ãƒ»å¹´é½¢å±¤ãƒ»ç–¾æ‚£éƒ¨ä½ã®å‰²åˆã‚’å…¨åˆ†å‰²ã§ä¿æŒ")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Ÿè¡Œ
    creator = JSONBasedStratifiedDatasetCreator(
        json_path=json_path,
        image_dir=image_dir,
        output_path=output_path,
        train_ratio=train_ratio,
        val_ratio=val_ratio,
        test1_ratio=test1_ratio,
        test2_ratio=test2_ratio,
        age_threshold=age_threshold
    )
    
    try:
        result = creator.create_dataset()
        
        if result:
            print(f"\nğŸ‰ JSONãƒ™ãƒ¼ã‚¹å±¤åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†!")
            print(f"ğŸ“ å‡ºåŠ›å…ˆ: {output_path}")
            print(f"\nğŸ“Š å„åˆ†å‰²ã®ç‰¹å¾´:")
            print(f"   âœ… å…¨åˆ†å‰²ã§ã‚¯ãƒ©ã‚¹æ¯”ç‡ï¼ˆè‰¯æ€§ vs æ‚ªæ€§ï¼‰ãŒç­‰ã—ã„")
            print(f"   âœ… å…¨åˆ†å‰²ã§å¹´é½¢å±¤æ¯”ç‡ãŒç­‰ã—ã„")
            print(f"   âœ… å…¨åˆ†å‰²ã§ç–¾æ‚£éƒ¨ä½æ¯”ç‡ãŒç­‰ã—ã„")
            print(f"   âœ… Test1ã¨Test2ãŒå‡ç­‰åˆ†å‰²ã•ã‚Œãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®ä¿¡é ¼æ€§å‘ä¸Š")
            print(f"   ğŸ¯ JSONãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãé«˜å“è³ªå±¤åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
        else:
            print(f"\nâŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå¤±æ•—")
            
    except Exception as e:
        print(f"\nâŒ ãƒ¡ã‚¤ãƒ³å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()