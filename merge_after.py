#!/usr/bin/env python3
"""
è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®trainã¨validationã‚’çµ±åˆã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
- ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãã®ã¾ã¾ã®åå‰ã§æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã«çµ±åˆ
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ãã®ã¾ã¾çµ±åˆ
"""

import os
import json
import shutil
from pathlib import Path
import argparse
from tqdm import tqdm

def load_json(file_path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return []

def save_json(data, file_path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"âœ“ Saved: {file_path} ({len(data)} entries)")

def merge_datasets(source_dirs, output_dir, dataset_name="merged_dataset"):
    """
    è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆ
    
    Args:
        source_dirs: ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒªã‚¹ãƒˆ
        output_dir: å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        dataset_name: çµ±åˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
    """
    
    print(f"ğŸ”„ Starting dataset merger: {dataset_name}")
    print(f"Source datasets: {len(source_dirs)}")
    print(f"Output directory: {output_dir}")
    print()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    (output_path / "train").mkdir(exist_ok=True)
    (output_path / "validation").mkdir(exist_ok=True)
    
    # çµ±åˆã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    merged_train_data = []
    merged_validation_data = []
    
    # çµ±è¨ˆæƒ…å ±
    stats = {
        "total_datasets": len(source_dirs),
        "train_images": 0,
        "validation_images": 0,
        "errors": 0,
        "skipped": 0
    }
    
    # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‡¦ç†
    for i, source_dir in enumerate(source_dirs):
        source_path = Path(source_dir)
        
        print(f"ğŸ“‚ Processing dataset {i+1}/{len(source_dirs)}: {source_path.name}")
        
        # train ã¨ validation ã®å‡¦ç†
        for split in ["train", "validation"]:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ¢ã™
            metadata_files = list(source_path.glob(f"{split}_metadata*.json"))
            if not metadata_files:
                print(f"  âš ï¸  No {split} metadata file found")
                continue
            
            metadata_file = metadata_files[0]  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
            print(f"  ğŸ“„ Loading {split} metadata: {metadata_file.name}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            metadata = load_json(metadata_file)
            if not metadata:
                continue
            
            # ç”»åƒãƒ•ã‚©ãƒ«ãƒ€
            images_dir = source_path / split
            if not images_dir.exists():
                print(f"  âš ï¸  Images directory not found: {images_dir}")
                continue
            
            processed_count = 0
            
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦ã‚³ãƒ”ãƒ¼
            for image_file in images_dir.iterdir():
                if image_file.is_file() and image_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']:
                    try:
                        # åŒåãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        output_image_path = output_path / split / image_file.name
                        if output_image_path.exists():
                            stats["skipped"] += 1
                            continue
                        
                        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼
                        shutil.copy2(image_file, output_image_path)
                        processed_count += 1
                        
                    except Exception as e:
                        print(f"    âœ— Error copying {image_file.name}: {e}")
                        stats["errors"] += 1
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾çµ±åˆ
            if split == "train":
                merged_train_data.extend(metadata)
                stats["train_images"] += processed_count
            else:
                merged_validation_data.extend(metadata)
                stats["validation_images"] += processed_count
            
            print(f"  âœ“ {split}: {processed_count} images copied")
        
        print()
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    print("ğŸ’¾ Saving merged metadata files...")
    save_json(merged_train_data, output_path / "train_metadata.json")
    save_json(merged_validation_data, output_path / "validation_metadata.json")
    
    # çµ±è¨ˆæƒ…å ±ã®ä¿å­˜
    stats_data = {
        "dataset_name": dataset_name,
        "source_datasets": [str(Path(d).name) for d in source_dirs],
        "statistics": stats,
        "output_structure": {
            "train_images": stats["train_images"],
            "validation_images": stats["validation_images"],
            "total_images": stats["train_images"] + stats["validation_images"]
        }
    }
    
    save_json(stats_data, output_path / "dataset_info.json")
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*60)
    print("ğŸ‰ DATASET MERGER COMPLETED!")
    print("="*60)
    print(f"ğŸ“Š Statistics:")
    print(f"  Source datasets: {stats['total_datasets']}")
    print(f"  Train images: {stats['train_images']:,}")
    print(f"  Validation images: {stats['validation_images']:,}")
    print(f"  Total images: {stats['train_images'] + stats['validation_images']:,}")
    print(f"  Skipped (duplicates): {stats['skipped']:,}")
    print(f"  Errors: {stats['errors']:,}")
    print(f"\nğŸ“ Output directory: {output_path}")
    print(f"  ğŸ“„ train_metadata.json")
    print(f"  ğŸ“„ validation_metadata.json")
    print(f"  ğŸ“„ dataset_info.json")
    print(f"  ğŸ“‚ train/ ({stats['train_images']} images)")
    print(f"  ğŸ“‚ validation/ ({stats['validation_images']} images)")

def main():
    parser = argparse.ArgumentParser(description="Merge multiple datasets into one")
    parser.add_argument(
        '--input', '-i', 
        nargs='+', 
        required=True,
        help='Input dataset directories'
    )
    parser.add_argument(
        '--output', '-o',
        required=True,
        help='Output directory'
    )
    parser.add_argument(
        '--name', '-n',
        default="merged_dataset",
        help='Dataset name (default: merged_dataset)'
    )
    
    args = parser.parse_args()
    
    # å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    print("ğŸ” Checking input directories...")
    valid_dirs = []
    
    for input_dir in args.input:
        input_path = Path(input_dir)
        if not input_path.exists():
            print(f"  âœ— Directory not found: {input_dir}")
            continue
        
        # train ã¾ãŸã¯ validation ãƒ•ã‚©ãƒ«ãƒ€ã®å­˜åœ¨ç¢ºèª
        has_train = (input_path / "train").exists()
        has_validation = (input_path / "validation").exists()
        
        if not (has_train or has_validation):
            print(f"  âš ï¸  No train/validation folders in: {input_dir}")
            continue
        
        valid_dirs.append(str(input_path))
        print(f"  âœ“ {input_path.name}")
    
    if not valid_dirs:
        print("âŒ No valid input directories found!")
        return
    
    print(f"\nğŸ“ Configuration:")
    print(f"  Input datasets: {len(valid_dirs)}")
    print(f"  Output directory: {args.output}")
    print(f"  Dataset name: {args.name}")
    
    # ç¢ºèª
    response = input("\nProceed with merging? (y/N): ")
    if response.lower() != 'y':
        print("âŒ Cancelled by user")
        return
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±åˆå®Ÿè¡Œ
    merge_datasets(valid_dirs, args.output, args.name)

if __name__ == "__main__":
    main()