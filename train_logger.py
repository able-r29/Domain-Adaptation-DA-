"""
DANN Training Logger - ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»å­¦ç¿’å†é–‹å¯¾å¿œç‰ˆ
"""

import os
import torch
import wandb
from ignite.engine import Events


def safe_get(eval_dict, key, default=0.0):
    """è¾æ›¸ã‹ã‚‰å®‰å…¨ã«ã‚­ãƒ¼ã‚’å–å¾—"""
    if eval_dict and isinstance(eval_dict, dict) and key in eval_dict:
        return eval_dict[key]
    else:
        return default


def create_logger_with_best_model_saving(trainer, classifier, domain_discriminator, eval_tr, eval_vl, 
                                        loader_eval_tr, loader_eval_vl, optimizer, scheduler, out_dir, config):
    """ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜æ©Ÿèƒ½ä»˜ããƒ­ã‚°å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ä½œæˆ"""
    
    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«è¿½è·¡å¤‰æ•°ï¼ˆã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£å†…ã§ä¿æŒï¼‰
    best_val_auc = 0.0
    best_epoch = 0
    best_model_info = {}
    
    @trainer.on(Events.EPOCH_COMPLETED)
    def log_results(engine):
        nonlocal best_val_auc, best_epoch, best_model_info
        
        out = engine.state.output
        epoch = engine.state.epoch
        
        print(f"Epoch {epoch:3d} - "
              f"Loss: {out['loss']:.4f} "
              f"(Cls: {out['cls_loss']:.4f}, Domain: {out['domain_loss']:.4f}) | "
              f"Alpha: {out['alpha']:.4f}, Domain Acc: {out['domain_acc']:.3f}")
        
        try:
            classifier.eval()
            domain_discriminator.eval()
            
            print("  ğŸ” Running training evaluation...")
            eval_tr.run(loader_eval_tr, max_epochs=1)
            train_eval = eval_tr.state.output
            
            print("  ğŸ” Running validation evaluation...")
            eval_vl.run(loader_eval_vl, max_epochs=1)
            val_eval = eval_vl.state.output
            
            print(f"  Train - Loss: {safe_get(train_eval, 'total_loss', 1.0):.4f}, "
                  f"Cls Acc: {safe_get(train_eval, 'cls_accuracy', 0.0):.3f}, "
                  f"Domain Acc: {safe_get(train_eval, 'domain_accuracy', 0.5):.3f}")
            
            print(f"  Val   - Loss: {safe_get(val_eval, 'total_loss', 1.0):.4f}, "
                  f"Cls Acc: {safe_get(val_eval, 'cls_accuracy', 0.0):.3f}, "
                  f"AUC: {safe_get(val_eval, 'cls_auc', 0.5):.3f}, "
                  f"Domain Acc: {safe_get(val_eval, 'domain_accuracy', 0.5):.3f}")
            
            # â˜… ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«åˆ¤å®šï¼ˆvalidation AUCã§åˆ¤å®šï¼‰
            current_val_auc = safe_get(val_eval, 'cls_auc', 0.0)
            
            if current_val_auc > best_val_auc:
                best_val_auc = current_val_auc
                best_epoch = epoch
                
                print(f"  ğŸ† NEW BEST MODEL! Val AUC: {current_val_auc:.4f} (previous: {best_val_auc:.4f})")
                
                # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¨˜éŒ²
                best_model_info = {
                    'epoch': epoch,
                    'val_auc': current_val_auc,
                    'val_acc': safe_get(val_eval, 'cls_accuracy', 0.0),
                    'domain_acc': safe_get(val_eval, 'domain_accuracy', 0.5),
                    'train_metrics': out,
                    'val_metrics': val_eval,
                }
                
                # â˜… ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
                best_model_dict = {
                    'epoch': epoch,
                    'best_val_auc': current_val_auc,
                    'val_cls_acc': safe_get(val_eval, 'cls_accuracy', 0.0),
                    'val_domain_acc': safe_get(val_eval, 'domain_accuracy', 0.5),
                    'classifier_state_dict': classifier.state_dict(),
                    'domain_discriminator_state_dict': domain_discriminator.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                    'config': config,
                    'train_metrics': out,
                    'val_metrics': val_eval,
                    'wandb_id': wandb.run.id if wandb.run else None,
                }
                
                best_model_path = os.path.join(out_dir, 'best_model.pth')
                torch.save(best_model_dict, best_model_path)
                print(f"    ğŸ’¾ Saved: best_model.pth")
            
            # ç¾åœ¨ã®ãƒ™ã‚¹ãƒˆæƒ…å ±ã‚’è¡¨ç¤º
            print(f"  ğŸ“Š Best: AUC {best_val_auc:.4f} at epoch {best_epoch}")
            
            # WandBãƒ­ã‚°ï¼ˆãƒ™ã‚¹ãƒˆæƒ…å ±è¿½åŠ ï¼‰
            wandb_log = {
                "epoch": epoch,
                "train/total_loss": out['loss'],
                "train/cls_loss": out['cls_loss'],
                "train/domain_loss": out['domain_loss'],
                "train/cls_acc": out['cls_acc'],
                "train/cls_auc": out['cls_auc'],
                "train/domain_acc": out['domain_acc'],
                "train/domain_auc": out['domain_auc'],
                "train/alpha": out['alpha'],
                "train_eval/total_loss": safe_get(train_eval, 'total_loss', 1.0),
                "train_eval/cls_acc": safe_get(train_eval, 'cls_accuracy', 0.0),
                "train_eval/cls_auc": safe_get(train_eval, 'cls_auc', 0.5),
                "train_eval/domain_acc": safe_get(train_eval, 'domain_accuracy', 0.5),
                "train_eval/domain_auc": safe_get(train_eval, 'domain_auc', 0.5),
                "val/total_loss": safe_get(val_eval, 'total_loss', 1.0),
                "val/cls_acc": safe_get(val_eval, 'cls_accuracy', 0.0),
                "val/cls_auc": safe_get(val_eval, 'cls_auc', 0.5),
                "val/domain_acc": safe_get(val_eval, 'domain_accuracy', 0.5),
                "val/domain_auc": safe_get(val_eval, 'domain_auc', 0.5),
                # â˜… ãƒ™ã‚¹ãƒˆæƒ…å ±è¿½åŠ 
                "best/val_auc": best_val_auc,
                "best/epoch": best_epoch,
                "best/val_acc": best_model_info.get('val_acc', 0.0),
                "best/domain_acc": best_model_info.get('domain_acc', 0.5),
            }
            
            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ 
            for eval_data, prefix in [(train_eval, "train_eval"), (val_eval, "val")]:
                if eval_data and 'target_cls_accuracy' in eval_data:
                    wandb_log[f"{prefix}/target_cls_acc"] = eval_data['target_cls_accuracy']
            
            wandb.log(wandb_log)
            
            classifier.train()
            domain_discriminator.train()
            
        except Exception as e:
            print(f"âš ï¸ Evaluation failed: {e}")
            try:
                wandb.log({
                    "epoch": epoch,
                    "train/total_loss": out['loss'],
                    "train/cls_loss": out['cls_loss'],
                    "train/domain_loss": out['domain_loss'],
                    "train/cls_acc": out['cls_acc'],
                    "train/alpha": out['alpha'],
                })
            except:
                pass
    
    # â˜… è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ï¼ˆä¸­æ–­å¯¾ç­–ï¼‰
    @trainer.on(Events.EPOCH_COMPLETED)
    def auto_save_checkpoint(engine):
        nonlocal best_val_auc, best_epoch, best_model_info
        
        epoch = engine.state.epoch
        
        # æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆæ¯ã‚¨ãƒãƒƒã‚¯æ›´æ–°ï¼‰
        checkpoint_dict = {
            'epoch': epoch,
            'classifier_state_dict': classifier.state_dict(),
            'domain_discriminator_state_dict': domain_discriminator.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'config': config,
            'best_val_auc': best_val_auc,
            'best_epoch': best_epoch,
            'best_model_info': best_model_info,
            'wandb_id': wandb.run.id if wandb.run else None,
        }
        
        # æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        latest_path = os.path.join(out_dir, 'latest_checkpoint.pth')
        torch.save(checkpoint_dict, latest_path)
        
        # 10ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ç•ªå·ä»˜ããƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        if epoch % 10 == 0:
            numbered_path = os.path.join(out_dir, f'checkpoint_epoch_{epoch}.pth')
            torch.save(checkpoint_dict, numbered_path)
            print(f"  ğŸ’¾ Checkpoint: epoch_{epoch}.pth")
    
    # â˜… ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¿”ã™é–¢æ•°ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã«ä½¿ç”¨ï¼‰
    def get_best_model_info():
        return {
            'best_val_auc': best_val_auc,
            'best_epoch': best_epoch,
            'best_model_info': best_model_info
        }
    
    return get_best_model_info


def create_logger(trainer, classifier, domain_discriminator, eval_tr, eval_vl, 
                 loader_eval_tr, loader_eval_vl):
    """å¾“æ¥ã®ãƒ­ã‚°å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆå¾Œæ–¹äº’æ›æ€§ç”¨ï¼‰"""
    
    @trainer.on(Events.EPOCH_COMPLETED)
    def log_results(engine):
        out = engine.state.output
        epoch = engine.state.epoch
        
        print(f"Epoch {epoch:3d} - "
              f"Loss: {out['loss']:.4f} "
              f"(Cls: {out['cls_loss']:.4f}, Domain: {out['domain_loss']:.4f}) | "
              f"Alpha: {out['alpha']:.4f}, Domain Acc: {out['domain_acc']:.3f}")
        
        try:
            classifier.eval()
            domain_discriminator.eval()
            
            print("  ğŸ” Running training evaluation...")
            eval_tr.run(loader_eval_tr, max_epochs=1)
            train_eval = eval_tr.state.output
            
            print("  ğŸ” Running validation evaluation...")
            eval_vl.run(loader_eval_vl, max_epochs=1)
            val_eval = eval_vl.state.output
            
            print(f"  Train - Loss: {safe_get(train_eval, 'total_loss', 1.0):.4f}, "
                  f"Cls Acc: {safe_get(train_eval, 'cls_accuracy', 0.0):.3f}, "
                  f"Domain Acc: {safe_get(train_eval, 'domain_accuracy', 0.5):.3f}")
            
            print(f"  Val   - Loss: {safe_get(val_eval, 'total_loss', 1.0):.4f}, "
                  f"Cls Acc: {safe_get(val_eval, 'cls_accuracy', 0.0):.3f}, "
                  f"Domain Acc: {safe_get(val_eval, 'domain_accuracy', 0.5):.3f}")
            
            # WandBãƒ­ã‚°
            wandb_log = {
                "epoch": epoch,
                "train/total_loss": out['loss'],
                "train/cls_loss": out['cls_loss'],
                "train/domain_loss": out['domain_loss'],
                "train/cls_acc": out['cls_acc'],
                "train/cls_auc": out['cls_auc'],
                "train/domain_acc": out['domain_acc'],
                "train/domain_auc": out['domain_auc'],
                "train/alpha": out['alpha'],
                "train_eval/total_loss": safe_get(train_eval, 'total_loss', 1.0),
                "train_eval/cls_acc": safe_get(train_eval, 'cls_accuracy', 0.0),
                "train_eval/cls_auc": safe_get(train_eval, 'cls_auc', 0.5),
                "train_eval/domain_acc": safe_get(train_eval, 'domain_accuracy', 0.5),
                "train_eval/domain_auc": safe_get(train_eval, 'domain_auc', 0.5),
                "val/total_loss": safe_get(val_eval, 'total_loss', 1.0),
                "val/cls_acc": safe_get(val_eval, 'cls_accuracy', 0.0),
                "val/cls_auc": safe_get(val_eval, 'cls_auc', 0.5),
                "val/domain_acc": safe_get(val_eval, 'domain_accuracy', 0.5),
                "val/domain_auc": safe_get(val_eval, 'domain_auc', 0.5),
            }
            
            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ 
            for eval_data, prefix in [(train_eval, "train_eval"), (val_eval, "val")]:
                if eval_data and 'target_cls_accuracy' in eval_data:
                    wandb_log[f"{prefix}/target_cls_acc"] = eval_data['target_cls_accuracy']
            
            wandb.log(wandb_log)
            
            classifier.train()
            domain_discriminator.train()
            
        except Exception as e:
            print(f"âš ï¸ Evaluation failed: {e}")
            try:
                wandb.log({
                    "epoch": epoch,
                    "train/total_loss": out['loss'],
                    "train/cls_loss": out['cls_loss'],
                    "train/domain_loss": out['domain_loss'],
                    "train/cls_acc": out['cls_acc'],
                    "train/alpha": out['alpha'],
                })
            except:
                pass


def setup_wandb(fold, out_dir, config, resume_id=None):
    """WandBåˆæœŸåŒ–ï¼ˆå†é–‹å¯¾å¿œï¼‰"""
    wandb.init(
        project="ResNet18_DANN_base",
        name=f"dann_base_fold{fold}" if fold is not None else "dann_base_holdout",
        config=config,
        dir=out_dir,
        tags=["dann", "resnet18_base"],
        resume="allow" if resume_id else None,
        id=resume_id
    )


def print_system_info(device_ids, primary_device, parallel_mode):
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º"""
    print(f"ğŸš€ Starting DANN Training (resnet18_base)")
    print(f"Device: {primary_device}, Parallel: {parallel_mode}")


def print_dataset_info(loader_src, loader_target, loader_eval_tr, loader_eval_vl):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±è¡¨ç¤º"""
    print(f"ğŸ“‚ Loading datasets...")
    print(f"  Source train batches: {len(loader_src)}")
    print(f"  Target train batches: {len(loader_target)}")
    print(f"  Source eval batches: {len(loader_eval_tr)}")
    print(f"  Source val batches: {len(loader_eval_vl)}")


def print_model_info(backbone, num_classes, bottleneck_dim, domain_hidden):
    """ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º"""
    print(f"ğŸ—ï¸ Building models...")
    print(f"  Backbone type: {type(backbone).__name__}")
    print(f"  Has feature method: {hasattr(backbone, 'feature')}")
    print(f"ğŸ“Š DANN Configuration:")
    print(f"  Num classes: {num_classes}")
    print(f"  Bottleneck dim: {bottleneck_dim}")
    print(f"  Domain hidden: {domain_hidden}")


def print_optimizer_info(optimizer, scheduler):
    """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼æƒ…å ±è¡¨ç¤º"""
    print(f"  Optimizer: {optimizer.__class__.__name__}")
    print(f"  Learning rate: {optimizer.param_groups[0]['lr']}")
    print(f"  Scheduler: {scheduler.__class__.__name__ if scheduler else 'None'}")