"""
Domain Discriminator for DANN (Domain-Adversarial Neural Networks)
"""

import torch
import torch.nn as nn
from torch.nn.parallel import DataParallel


class GradientReverseFunction(torch.autograd.Function):
    """å‹¾é…åè»¢ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å®Ÿè£…"""
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    
    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.alpha, None


class GradientReverseLayer(nn.Module):
    """å‹¾é…åè»¢ãƒ¬ã‚¤ãƒ¤ãƒ¼"""
    def __init__(self, alpha=1.0):
        super(GradientReverseLayer, self).__init__()
        self.alpha = alpha
    
    def forward(self, x):
        return GradientReverseFunction.apply(x, self.alpha)
    
    def set_alpha(self, alpha):
        self.alpha = alpha


class DomainDiscriminator(nn.Module):
    """ãƒ‰ãƒ¡ã‚¤ãƒ³è­˜åˆ¥å™¨ï¼ˆå‹•çš„ç‰¹å¾´é‡æ¬¡å…ƒå¯¾å¿œï¼‰"""
    def __init__(self, initial_feature_dim=512, hidden_dim=1024):
        super(DomainDiscriminator, self).__init__()
        self.grl = GradientReverseLayer()
        self.initial_feature_dim = initial_feature_dim
        self.hidden_dim = hidden_dim
        self.current_feature_dim = None
        self.classifier = None
        
        print(f"ğŸ—ï¸ DomainDiscriminator initialized with initial_feature_dim={initial_feature_dim}")
    
    def _create_classifier(self, feature_dim, device):
        """ç‰¹å¾´é‡æ¬¡å…ƒã«åŸºã¥ã„ã¦åˆ†é¡å™¨ã‚’å‹•çš„ã«ä½œæˆ"""
        if self.current_feature_dim != feature_dim:
            print(f"ğŸ”§ Creating domain discriminator layers for feature dim: {feature_dim}")
            
            self.current_feature_dim = feature_dim
            self.classifier = nn.Sequential(
                nn.Linear(feature_dim, self.hidden_dim),
                nn.BatchNorm1d(self.hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(self.hidden_dim, self.hidden_dim),
                nn.BatchNorm1d(self.hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(self.hidden_dim, 1),
                nn.Sigmoid()
            ).to(device)
            
            print(f"âœ“ Domain discriminator layers created: {feature_dim} -> {self.hidden_dim} -> 1")
    
    def forward(self, x):
        # â˜… ã“ã“ã§å‹¾é…åè»¢ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’é€šéï¼ˆé‡è¦ï¼‰
        x = self.grl(x)  # é †ä¼æ’­ï¼šãã®ã¾ã¾é€šã™ / é€†ä¼æ’­ï¼šå‹¾é…ã‚’åè»¢
        
        # å‹•çš„ã«åˆ†é¡å™¨ã‚’ä½œæˆ
        current_feature_dim = x.size(1)
        if self.classifier is None or self.current_feature_dim != current_feature_dim:
            self._create_classifier(current_feature_dim, x.device)
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³è­˜åˆ¥
        return self.classifier(x)
    
    def set_alpha(self, alpha):
        """GRLã®alphaå€¤ã‚’è¨­å®š"""
        self.grl.set_alpha(alpha)  # Î±=1.0ã«è¨­å®š


class DANNClassifier(nn.Module):
    """DANNç”¨åˆ†é¡å™¨ï¼ˆresnet18_mtpå¯¾å¿œãƒ»ä¿®æ­£ç‰ˆï¼‰"""
    def __init__(self, backbone, num_classes, bottleneck_dim=256):
        super(DANNClassifier, self).__init__()
        self.backbone = backbone
        self.num_classes = num_classes
        self.bottleneck_dim = bottleneck_dim
        self.current_feature_dim = None
        self.bottleneck = None
        self.classifier = None
        
        print(f"ğŸ—ï¸ DANNClassifier initialized:")
        print(f"  Backbone: {type(backbone).__name__}")
        print(f"  Bottleneck dim: {bottleneck_dim}")
        print(f"  Num classes: {num_classes}")
        
    def _create_layers(self, feature_dim, device):
        """ç‰¹å¾´é‡æ¬¡å…ƒã«åŸºã¥ã„ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å±¤ã¨åˆ†é¡ãƒ˜ãƒƒãƒ‰ã‚’å‹•çš„ã«ä½œæˆ"""
        if self.current_feature_dim != feature_dim:
            print(f"ğŸ”§ Creating classifier layers for feature dim: {feature_dim}")
            
            self.current_feature_dim = feature_dim
            
            # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å±¤ã‚’ä½œæˆ
            self.bottleneck = nn.Sequential(
                nn.Linear(feature_dim, self.bottleneck_dim),
                nn.BatchNorm1d(self.bottleneck_dim),
                nn.ReLU(),
                nn.Dropout(0.5)
            ).to(device)
            
            # åˆ†é¡ãƒ˜ãƒƒãƒ‰ã‚’ä½œæˆ
            self.classifier = nn.Linear(self.bottleneck_dim, self.num_classes).to(device)
            
            print(f"âœ“ Classifier layers created: {feature_dim} -> {self.bottleneck_dim} -> {self.num_classes}")
    
    def _prepare_backbone_input(self, x):
        """ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ç”¨ã®å…¥åŠ›ã‚’æº–å‚™ï¼ˆresnet18_mtpå¯¾å¿œï¼‰"""
        # resnet18_mtpã¯è¾æ›¸å½¢å¼ã‚’æœŸå¾…: {'anchor': tensor, 'positive': tensor, 'meta_a': tensor, 'meta_p': tensor}
        
        if isinstance(x, dict):
            # æ—¢ã«è¾æ›¸å½¢å¼ã®å ´åˆ
            if 'anchor' in x:
                # å®Œå…¨ãªè¾æ›¸ãŒæ¸¡ã•ã‚ŒãŸå ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
                return x
            else:
                # anchorã‚­ãƒ¼ãŒãªã„å ´åˆã¯æœ€åˆã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’anchorã¨ã—ã¦ä½¿ç”¨
                tensor_keys = [k for k, v in x.items() if isinstance(v, torch.Tensor)]
                if tensor_keys:
                    anchor_tensor = x[tensor_keys[0]]
                    # ãƒ€ãƒŸãƒ¼ã®metaæƒ…å ±ã‚’ä½œæˆï¼ˆresnet18_mtpãŒè¦æ±‚ã™ã‚‹å½¢å¼ï¼‰
                    batch_size = anchor_tensor.size(0)
                    device = anchor_tensor.device
                    
                    return {
                        'anchor': anchor_tensor,
                        'positive': anchor_tensor,  # ãƒ€ãƒŸãƒ¼ï¼šanchorã¨åŒã˜
                        'meta_a': torch.zeros(batch_size, 1, device=device),  # ãƒ€ãƒŸãƒ¼ãƒ¡ã‚¿æƒ…å ±
                        'meta_p': torch.zeros(batch_size, 1, device=device)   # ãƒ€ãƒŸãƒ¼ãƒ¡ã‚¿æƒ…å ±
                    }
                else:
                    raise ValueError(f"No tensor found in input dict: {list(x.keys())}")
        
        elif isinstance(x, torch.Tensor):
            # 4æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ«ãŒç›´æ¥æ¸¡ã•ã‚ŒãŸå ´åˆ
            batch_size = x.size(0)
            device = x.device
            
            return {
                'anchor': x,
                'positive': x,  # ãƒ€ãƒŸãƒ¼ï¼šanchorã¨åŒã˜
                'meta_a': torch.zeros(batch_size, 1, device=device),  # ãƒ€ãƒŸãƒ¼ãƒ¡ã‚¿æƒ…å ±
                'meta_p': torch.zeros(batch_size, 1, device=device)   # ãƒ€ãƒŸãƒ¼ãƒ¡ã‚¿æƒ…å ±
            }
        else:
            raise ValueError(f"Unexpected input type: {type(x)}")
        
    def forward(self, x):
        # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ç”¨ã®å…¥åŠ›ã‚’æº–å‚™
        backbone_input = self._prepare_backbone_input(x)
        
        # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã§ç‰¹å¾´æŠ½å‡ºï¼ˆresnet18_mtpã®featureãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼‰
        if hasattr(self.backbone, 'feature'):
            # featureãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚‹å ´åˆï¼ˆanchorã®ã¿ã®ç‰¹å¾´æŠ½å‡ºï¼‰
            features = self.backbone.feature(backbone_input)
        else:
            # é€šå¸¸ã®forwardã‚’ä½¿ç”¨ã—ã¦anchorç‰¹å¾´é‡ã‚’æŠ½å‡º
            try:
                output = self.backbone(backbone_input)
                if isinstance(output, dict) and 'ya' in output:
                    # resnet18_mtpã®å ´åˆã€predictãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦featureã‚’å–å¾—
                    if hasattr(self.backbone, 'predict'):
                        _, features = self.backbone.predict(backbone_input)
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šyaã®å‰ã®å±¤ã®å‡ºåŠ›ã‚’å–å¾—
                        features = self.backbone.feature(backbone_input)
                else:
                    features = output
            except Exception as e:
                print(f"âš ï¸ Backbone forward failed: {e}")
                # æœ€å¾Œã®æ‰‹æ®µï¼šfeatureãƒ¡ã‚½ãƒƒãƒ‰ã‚’è©¦è¡Œ
                features = self.backbone.feature(backbone_input)
        
        # ç‰¹å¾´é‡ã‚’å¹³å¦åŒ–
        if len(features.shape) > 2:
            features = features.view(features.size(0), -1)
        
        # å‹•çš„ã«å±¤ã‚’ä½œæˆ
        current_feature_dim = features.size(1)
        if self.bottleneck is None or self.current_feature_dim != current_feature_dim:
            self._create_layers(current_feature_dim, features.device)
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å±¤ã‚’é€šã™
        bottleneck_features = self.bottleneck(features)
        
        # åˆ†é¡
        logits = self.classifier(bottleneck_features)
        
        return logits, bottleneck_features


def calculate_lambda_p(epoch, max_epochs):
    """å­¦ç¿’é€²è¡Œã«å¿œã˜ã¦GRLã®å¼·åº¦ã‚’èª¿æ•´"""
    import numpy as np
    p = float(epoch) / max_epochs
    return 2.0 / (1.0 + np.exp(-10 * p)) - 1.0