"""
Domain Discriminator for DANN (resnet18_baseå¯¾å¿œç‰ˆ)
"""

import torch
import torch.nn as nn


class GradientReverseFunction(torch.autograd.Function):
    """å‹¾é…åè»¢ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å®Ÿè£…"""
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    
    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.alpha, None


class GradientReverseLayer(nn.Module):
    """å‹¾é…åè»¢ãƒ¬ã‚¤ãƒ¤ãƒ¼"""
    def __init__(self, alpha=1.0):
        super(GradientReverseLayer, self).__init__()
        self.alpha = alpha
    
    def forward(self, x):
        return GradientReverseFunction.apply(x, self.alpha)
    
    def set_alpha(self, alpha):
        self.alpha = alpha


class DomainDiscriminator(nn.Module):
    """ãƒ‰ãƒ¡ã‚¤ãƒ³è­˜åˆ¥å™¨ï¼ˆresnet18_baseç”¨ãƒ»256æ¬¡å…ƒç‰¹å¾´é‡å¯¾å¿œï¼‰"""
    def __init__(self, feature_dim=256, hidden_dim=1024):
        super(DomainDiscriminator, self).__init__()
        self.grl = GradientReverseLayer()
        
        # å›ºå®šã•ã‚ŒãŸæ§‹é€ ï¼ˆresnet18_baseç”¨ï¼‰
        self.classifier = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        print(f"ğŸ—ï¸ DomainDiscriminator initialized:")
        print(f"  Input feature dim: {feature_dim}")
        print(f"  Hidden dim: {hidden_dim}")
        print(f"  Architecture: {feature_dim} -> {hidden_dim} -> {hidden_dim} -> 1")
    
    def forward(self, x):
        # â˜… å‹¾é…åè»¢ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’é€šéï¼ˆé‡è¦ï¼‰
        x = self.grl(x)
        return self.classifier(x)
    
    def set_alpha(self, alpha):
        """GRLã®alphaå€¤ã‚’è¨­å®š"""
        self.grl.set_alpha(alpha)


class DANNClassifier(nn.Module):
    """DANNç”¨åˆ†é¡å™¨ï¼ˆresnet18_baseå°‚ç”¨ãƒ»ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
    def __init__(self, backbone, num_classes, bottleneck_dim=256):
        super(DANNClassifier, self).__init__()
        self.backbone = backbone
        self.num_classes = num_classes
        self.bottleneck_dim = bottleneck_dim
        
        # resnet18_baseã¯512æ¬¡å…ƒç‰¹å¾´é‡ã‚’å‡ºåŠ›
        self.bottleneck = nn.Sequential(
            nn.Linear(512, bottleneck_dim),
            nn.BatchNorm1d(bottleneck_dim),
            nn.ReLU(),
            nn.Dropout(0.5)
        )
        
        # åˆ†é¡ãƒ˜ãƒƒãƒ‰
        self.classifier_head = nn.Linear(bottleneck_dim, num_classes)
        
        print(f"ğŸ—ï¸ DANNClassifier initialized:")
        print(f"  Backbone: resnet18_base (512 features)")
        print(f"  Bottleneck: 512 -> {bottleneck_dim}")
        print(f"  Classifier: {bottleneck_dim} -> {num_classes}")
    
    def forward(self, x):
        """
        Args:
            x: ç”»åƒãƒ†ãƒ³ã‚½ãƒ« [batch_size, 3, height, width]
        Returns:
            logits: åˆ†é¡ã‚¹ã‚³ã‚¢ [batch_size, num_classes]
            bottleneck_features: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å¾´ [batch_size, bottleneck_dim]
        """
        # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã§ç‰¹å¾´æŠ½å‡ºï¼ˆ512æ¬¡å…ƒï¼‰
        backbone_features = self.backbone.feature(x)
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å±¤ã‚’é€šã™ï¼ˆ256æ¬¡å…ƒï¼‰
        bottleneck_features = self.bottleneck(backbone_features)
        
        # åˆ†é¡
        logits = self.classifier_head(bottleneck_features)
        
        return logits, bottleneck_features


def calculate_lambda_p(epoch, max_epochs):
    """å­¦ç¿’é€²è¡Œã«å¿œã˜ã¦GRLã®å¼·åº¦ã‚’èª¿æ•´"""
    import numpy as np
    p = float(epoch) / max_epochs
    return 2.0 / (1.0 + np.exp(-10 * p)) - 1.0