#!/usr/bin/env python3
"""
Hanley McNeilæ³•ã«ã‚ˆã‚‹AUCæ¤œå®šãƒ—ãƒ­ã‚°ãƒ©ãƒ 
2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®AUCå€¤ã®æœ‰æ„å·®ã‚’æ¤œå®šã—ã¾ã™
"""

import argparse
import json
import pickle
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# ==========================================
# ğŸ‘‡ ã“ã“ã§ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç›´æ¥æŒ‡å®šã—ã¦ãã ã•ã„
# ==========================================
DEFAULT_PATHS = {
    # åŸºæº–ã¨ãªã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆå›ºå®šï¼‰
    'reference': '../resnet18_before_gamma4/before_test1/predict_detailed_results.csv',
    'reference_name': 'before_test1',
    
    # æ¯”è¼ƒå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦ .csv ã«å¤‰æ›´ï¼‰
    'models': [
        {'path': '../resnet18_before_gamma4/all_after_test1/predict_detailed_results.csv', 'name': 'all_after_test1'},
        {'path': '../resnet18_before_gamma4/KMOface/predict_detailed_results.csv', 'name': 'KMOface'},
        {'path': '../resnet18_before_gamma4/KMUbody/predict_detailed_results.csv', 'name': 'KMUbody'},
        {'path': '../resnet18_before_gamma4/KSObody/predict_detailed_results.csv', 'name': 'KSObody'},
        {'path': '../resnet18_before_gamma4/SSObody/predict_detailed_results.csv', 'name': 'SSObody'},
        {'path': '../resnet18_before_gamma4/SSOface/predict_detailed_results.csv', 'name': 'SSOface'},
        {'path': '../resnet18_before_gamma4/SSUbody/predict_detailed_results.csv', 'name': 'SSUbody'},
        {'path': '../resnet18_before_gamma4/YNObody/predict_detailed_results.csv', 'name': 'YNObody'},
    ],
    
    'output': './hanley_mcneil_multi_results',
    'alpha': 0.05,
    'bootstrap': True
}

def load_prediction_results(file_path):
    """äºˆæ¸¬çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆpickle, CSV, NPZå¯¾å¿œï¼‰"""
    print(f"Loading: {file_path}")
    
    file_path = Path(file_path)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    if not file_path.exists():
        raise FileNotFoundError(f"File not found: {file_path}")
    
    if file_path.suffix.lower() == '.csv':
        # CSVå½¢å¼ã®èª­ã¿è¾¼ã¿
        df = pd.read_csv(file_path)
        print(f"  CSV shape: {df.shape}")
        print(f"  CSV columns: {list(df.columns)}")
        
        # çœŸã®ãƒ©ãƒ™ãƒ«ã‚’ç‰¹å®šï¼ˆtrue_classã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ï¼‰
        if 'true_class' in df.columns:
            true_labels = df['true_class'].values
            print(f"  Using label column: true_class")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            label_columns = [col for col in df.columns if col.lower() in 
                            ['true_label', 'label', 'ground_truth', 'gt', 'y_true', 'actual']]
            if not label_columns:
                raise ValueError(f"No label column found. Available columns: {list(df.columns)}")
            true_labels = df[label_columns[0]].values
            print(f"  Using label column: {label_columns[0]}")
        
        # ç¢ºç‡åˆ—ã‚’ç‰¹å®šï¼ˆprob_class_0, prob_class_1ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ï¼‰
        prob_columns = [col for col in df.columns if col.startswith('prob_class_')]
        
        if len(prob_columns) >= 2:
            # prob_class_0, prob_class_1ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
            prob_columns_sorted = sorted(prob_columns)  # prob_class_0, prob_class_1ã®é †åºã‚’ä¿è¨¼
            predictions = df[prob_columns_sorted].values
            print(f"  Using probability columns: {prob_columns_sorted}")
            print(f"  âœ“ These are already normalized probabilities from CSV")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šä»–ã®äºˆæ¸¬åˆ—ã‚’æ¢ã™
            pred_columns = [col for col in df.columns if any(keyword in col.lower() for keyword in 
                           ['pred', 'prob', 'score', 'logit'])]
            
            if len(pred_columns) >= 2:
                predictions = df[pred_columns].values
                print(f"  Using prediction columns: {pred_columns}")
            elif len(pred_columns) == 1:
                predictions = df[pred_columns[0]].values.reshape(-1, 1)
                print(f"  Using single prediction column: {pred_columns[0]}")
            else:
                raise ValueError("No prediction columns found")
        
        file_paths = df.get('file_path', None)
        
        # ç¢ºç‡å€¤ã®æ¤œè¨¼
        if predictions.shape[1] == 2:
            row_sums = np.sum(predictions, axis=1)
            print(f"  First 5 probability pairs: {predictions[:5]}")
            print(f"  Row sums (first 5): {row_sums[:5]}")
            print(f"  Min probability: {np.min(predictions):.6f}")
            print(f"  Max probability: {np.max(predictions):.6f}")
        
    elif file_path.suffix.lower() in ['.pkl', '.pickle']:
        # Pickleå½¢å¼ã®èª­ã¿è¾¼ã¿
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        print(f"  Pickle content type: {type(data)}")
        
        if isinstance(data, tuple) and len(data) >= 2:
            predictions = data[0]
            true_labels = data[1]
            file_paths = data[2] if len(data) > 2 else None
            
        elif isinstance(data, list):
            print(f"  List length: {len(data)}")
            if len(data) > 0:
                print(f"  First element type: {type(data[0])}")
                
                if isinstance(data[0], dict):
                    predictions = np.array([item.get('prediction', item.get('pred', 0)) for item in data])
                    true_labels = np.array([item.get('true_label', item.get('label', 0)) for item in data])
                    file_paths = [item.get('file_path', f'sample_{i}') for i in range(len(data))]
                    
                elif isinstance(data[0], (list, tuple)) and len(data[0]) >= 2:
                    predictions = np.array([item[0] for item in data])
                    true_labels = np.array([item[1] for item in data])
                    file_paths = [item[2] if len(item) > 2 else f'sample_{i}' for i in range(len(data))]
                    
                else:
                    raise ValueError(f"Unsupported list content type: {type(data[0])}")
            else:
                raise ValueError("Empty list in pickle file")
                
        elif isinstance(data, dict):
            print(f"  Dictionary keys: {list(data.keys())}")
            predictions = data.get('predictions', data.get('pred', data.get('scores')))
            true_labels = data.get('true_labels', data.get('labels', data.get('y_true')))
            file_paths = data.get('file_paths', data.get('files'))
            
            if predictions is None or true_labels is None:
                raise ValueError(f"Required keys not found. Available: {list(data.keys())}")
                
        else:
            raise ValueError(f"Unexpected pickle format: {type(data)}")
    
    elif file_path.suffix.lower() == '.npz':
        # NPZå½¢å¼ã®èª­ã¿è¾¼ã¿
        data = np.load(file_path, allow_pickle=True)
        print(f"  NPZ arrays: {list(data.keys())}")
        
        predictions = data.get('predictions', data.get('pred', data.get('scores')))
        true_labels = data.get('true_labels', data.get('labels', data.get('y_true')))
        file_paths = data.get('file_paths', data.get('files', None))
        
        if predictions is None or true_labels is None:
            raise ValueError(f"Required arrays not found. Available: {list(data.keys())}")
    
    else:
        raise ValueError(f"Unsupported file format: {file_path.suffix}")
    
    # NumPyé…åˆ—ã«å¤‰æ›
    predictions = np.array(predictions)
    true_labels = np.array(true_labels)
    
    print(f"  Predictions shape: {predictions.shape}")
    print(f"  True labels shape: {true_labels.shape}")
    print(f"  Positive cases: {np.sum(true_labels == 1)}")
    print(f"  Negative cases: {np.sum(true_labels == 0)}")
    print(f"  Unique labels: {np.unique(true_labels)}")
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if len(predictions) != len(true_labels):
        raise ValueError(f"Predictions and labels length mismatch: {len(predictions)} vs {len(true_labels)}")
    
    return predictions, true_labels, file_paths

def softmax(logits):
    """Softmaxé–¢æ•°"""
    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))
    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)

def calculate_auc_variance(y_true, y_scores):
    """Hanley McNeilæ³•ã«ã‚ˆã‚‹AUCã®åˆ†æ•£è¨ˆç®—"""
    auc_value = roc_auc_score(y_true, y_scores)
    
    n_pos = np.sum(y_true == 1)
    n_neg = np.sum(y_true == 0)
    
    if n_pos == 0 or n_neg == 0:
        return auc_value, float('inf')
    
    Q1 = auc_value / (2 - auc_value)
    Q2 = (2 * auc_value ** 2) / (1 + auc_value)
    
    auc_variance = (auc_value * (1 - auc_value) + 
                   (n_pos - 1) * (Q1 - auc_value ** 2) + 
                   (n_neg - 1) * (Q2 - auc_value ** 2)) / (n_pos * n_neg)
    
    return auc_value, auc_variance

def hanley_mcneil_independent_test(y_true1, scores1, y_true2, scores2, alpha=0.05):
    """ç‹¬ç«‹ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã™ã‚‹Hanley McNeilæ³•ã«ã‚ˆã‚‹2ã¤ã®AUCã®æ¯”è¼ƒæ¤œå®š"""
    auc1, var1 = calculate_auc_variance(y_true1, scores1)
    auc2, var2 = calculate_auc_variance(y_true2, scores2)
    
    correlation = 0.0
    var_diff = var1 + var2
    
    if var_diff <= 0:
        z_score = float('inf')
        p_value = 0.0
    else:
        z_score = (auc1 - auc2) / np.sqrt(var_diff)
        p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))
    
    ci_lower = (auc1 - auc2) - stats.norm.ppf(1 - alpha/2) * np.sqrt(var_diff)
    ci_upper = (auc1 - auc2) + stats.norm.ppf(1 - alpha/2) * np.sqrt(var_diff)
    
    return {
        'auc1': auc1,
        'auc2': auc2,
        'auc_diff': auc1 - auc2,
        'var1': var1,
        'var2': var2,
        'correlation': correlation,
        'var_diff': var_diff,
        'z_score': z_score,
        'p_value': p_value,
        'alpha': alpha,
        'significant': p_value < alpha,
        'ci_lower': ci_lower,
        'ci_upper': ci_upper,
        'better_performance': 'Reference' if auc1 > auc2 else 'Comparison' if auc2 > auc1 else 'Equal',
        'sample_size1': len(y_true1),
        'sample_size2': len(y_true2),
        'test_type': 'Independent samples Hanley-McNeil test'
    }

def bootstrap_auc_comparison_independent(y_true1, scores1, y_true2, scores2, n_bootstrap=1000, alpha=0.05):
    """ç‹¬ç«‹ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã™ã‚‹ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ³•ã«ã‚ˆã‚‹AUCæ¯”è¼ƒ"""
    auc_diffs = []
    
    for _ in range(n_bootstrap):
        indices1 = np.random.choice(len(y_true1), size=len(y_true1), replace=True)
        indices2 = np.random.choice(len(y_true2), size=len(y_true2), replace=True)
        
        y_boot1 = y_true1[indices1]
        s_boot1 = scores1[indices1]
        y_boot2 = y_true2[indices2]
        s_boot2 = scores2[indices2]
        
        try:
            auc1_boot = roc_auc_score(y_boot1, s_boot1)
            auc2_boot = roc_auc_score(y_boot2, s_boot2)
            auc_diffs.append(auc1_boot - auc2_boot)
        except ValueError:
            continue
    
    auc_diffs = np.array(auc_diffs)
    
    ci_lower = np.percentile(auc_diffs, 100 * alpha/2)
    ci_upper = np.percentile(auc_diffs, 100 * (1 - alpha/2))
    
    observed_diff = roc_auc_score(y_true1, scores1) - roc_auc_score(y_true2, scores2)
    if observed_diff >= 0:
        p_value_boot = 2 * np.mean(auc_diffs <= 0)
    else:
        p_value_boot = 2 * np.mean(auc_diffs >= 0)
    
    return {
        'auc_diff_mean': np.mean(auc_diffs),
        'auc_diff_std': np.std(auc_diffs),
        'ci_lower_boot': ci_lower,
        'ci_upper_boot': ci_upper,
        'p_value_boot': p_value_boot,
        'significant_boot': not (ci_lower <= 0 <= ci_upper)
    }

def create_multi_comparison_plot(reference_data, comparison_data, save_path):
    """è¤‡æ•°æ¯”è¼ƒã®çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ"""
    ref_true, ref_scores, ref_name = reference_data
    
    n_comparisons = len(comparison_data)
    fig_height = max(10, 4 * n_comparisons)
    fig, axes = plt.subplots(n_comparisons, 3, figsize=(18, fig_height))
    
    if n_comparisons == 1:
        axes = axes.reshape(1, -1)
    
    ref_auc = roc_auc_score(ref_true, ref_scores)
    ref_fpr, ref_tpr, _ = roc_curve(ref_true, ref_scores)
    
    for i, (comp_true, comp_scores, comp_name) in enumerate(comparison_data):
        comp_auc = roc_auc_score(comp_true, comp_scores)
        comp_fpr, comp_tpr, _ = roc_curve(comp_true, comp_scores)
        
        # ROCæ›²ç·šæ¯”è¼ƒ
        axes[i, 0].plot(ref_fpr, ref_tpr, 'b-', lw=2, 
                       label=f'{ref_name} (AUC = {ref_auc:.4f})')
        axes[i, 0].plot(comp_fpr, comp_tpr, 'r-', lw=2, 
                       label=f'{comp_name} (AUC = {comp_auc:.4f})')
        axes[i, 0].plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)
        axes[i, 0].set_xlim([0.0, 1.0])
        axes[i, 0].set_ylim([0.0, 1.05])
        axes[i, 0].set_xlabel('False Positive Rate')
        axes[i, 0].set_ylabel('True Positive Rate')
        axes[i, 0].set_title(f'ROC: {ref_name} vs {comp_name}')
        axes[i, 0].legend(loc="lower right")
        axes[i, 0].grid(True, alpha=0.3)
        
        # AUCæ£’ã‚°ãƒ©ãƒ•
        models = [ref_name, comp_name]
        aucs = [ref_auc, comp_auc]
        colors = ['blue', 'red']
        
        bars = axes[i, 1].bar(models, aucs, color=colors, alpha=0.7)
        axes[i, 1].set_ylabel('AUC')
        axes[i, 1].set_title(f'AUC Comparison')
        axes[i, 1].set_ylim([0.0, 1.0])
        axes[i, 1].grid(True, alpha=0.3)
        
        for bar, auc in zip(bars, aucs):
            height = bar.get_height()
            axes[i, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{auc:.3f}', ha='center', va='bottom')
        
        # ã‚¹ã‚³ã‚¢åˆ†å¸ƒæ¯”è¼ƒ
        axes[i, 2].hist(ref_scores[ref_true==1], bins=30, alpha=0.5, 
                       label=f'{ref_name} (Pos)', color='blue')
        axes[i, 2].hist(comp_scores[comp_true==1], bins=30, alpha=0.5, 
                       label=f'{comp_name} (Pos)', color='red')
        axes[i, 2].set_xlabel('Prediction Score')
        axes[i, 2].set_ylabel('Frequency')
        axes[i, 2].set_title(f'Score Distribution (Positive Cases)')
        axes[i, 2].legend()
        axes[i, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

def create_summary_plot(results_summary, save_path):
    """å…¨çµæœã‚µãƒãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    models = [r['model_name'] for r in results_summary]
    aucs = [r['auc'] for r in results_summary]
    p_values = [r['p_value'] for r in results_summary]
    significant = [r['significant'] for r in results_summary]
    
    # AUCæ¯”è¼ƒ
    colors = ['blue' if i == 0 else ('red' if sig else 'gray') 
              for i, sig in enumerate(significant)]
    bars = ax1.bar(models, aucs, color=colors, alpha=0.7)
    ax1.set_ylabel('AUC')
    ax1.set_title('AUC Comparison (Blue: Reference, Red: Significant)')
    ax1.tick_params(axis='x', rotation=45)
    ax1.grid(True, alpha=0.3)
    
    for bar, auc in zip(bars, aucs):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{auc:.3f}', ha='center', va='bottom', fontsize=8)
    
    # På€¤åˆ†å¸ƒ
    comparison_models = models[1:]
    comparison_pvals = p_values[1:]
    
    bars = ax2.bar(comparison_models, comparison_pvals, 
                   color=['red' if pv < 0.05 else 'gray' for pv in comparison_pvals], 
                   alpha=0.7)
    ax2.axhline(y=0.05, color='red', linestyle='--', label='Î± = 0.05')
    ax2.set_ylabel('P-value')
    ax2.set_title('P-values vs Reference Model')
    ax2.tick_params(axis='x', rotation=45)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    for bar, pv in zip(bars, comparison_pvals):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{pv:.3f}', ha='center', va='bottom', fontsize=8)
    
    # æœ‰æ„æ€§ã‚µãƒãƒªãƒ¼
    sig_counts = [sum(significant[1:]), len(significant) - 1 - sum(significant[1:])]
    labels = ['Significant', 'Non-significant']
    colors_pie = ['red', 'gray']
    
    ax3.pie(sig_counts, labels=labels, colors=colors_pie, autopct='%1.0f%%')
    ax3.set_title('Significance Summary')
    
    # AUCå·®åˆ†
    ref_auc = aucs[0]
    auc_diffs = [auc - ref_auc for auc in aucs[1:]]
    
    bars = ax4.bar(comparison_models, auc_diffs,
                   color=['red' if sig else 'gray' for sig in significant[1:]],
                   alpha=0.7)
    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.5)
    ax4.set_ylabel('AUC Difference from Reference')
    ax4.set_title('AUC Differences')
    ax4.tick_params(axis='x', rotation=45)
    ax4.grid(True, alpha=0.3)
    
    for bar, diff in zip(bars, auc_diffs):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., 
                height + (0.001 if height >= 0 else -0.001),
                f'{diff:+.3f}', ha='center', 
                va='bottom' if height >= 0 else 'top', fontsize=8)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

def save_multi_results(all_results, results_summary, output_path):
    """è¤‡æ•°æ¯”è¼ƒçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.generic):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(item) for item in obj]
        else:
            return obj
    
    detailed_results = convert_numpy(all_results)
    with open(output_path + '_detailed_results.json', 'w') as f:
        json.dump(detailed_results, f, indent=2)
    
    summary_df = pd.DataFrame(results_summary)
    summary_df.to_csv(output_path + '_summary.csv', index=False)
    
    with open(output_path + '_multi_comparison_report.txt', 'w') as f:
        f.write("=== Multi-Model Hanley McNeil AUC Comparison Report ===\n\n")
        
        f.write("SUMMARY TABLE:\n")
        f.write("-" * 80 + "\n")
        f.write(f"{'Model':<25} {'AUC':<10} {'P-value':<10} {'Significant':<12} {'95% CI':<20}\n")
        f.write("-" * 80 + "\n")
        
        for result in results_summary:
            # Noneå€¤ã‚’é©åˆ‡ã«å‡¦ç†
            p_value_str = f"{result['p_value']:.4f}" if result['p_value'] is not None else "N/A"
            ci_str = f"[{result['ci_lower']:.3f}, {result['ci_upper']:.3f}]" if result['ci_lower'] is not None else "N/A"
            
            f.write(f"{result['model_name']:<25} {result['auc']:<10.4f} "
                   f"{p_value_str:<10} {str(result['significant']):<12} {ci_str:<20}\n")
        
        f.write("-" * 80 + "\n\n")
        
        significant_count = sum([r['significant'] for r in results_summary[1:]])
        total_comparisons = len(results_summary) - 1
        
        f.write("STATISTICAL SIGNIFICANCE SUMMARY:\n")
        f.write(f"Total comparisons: {total_comparisons}\n")
        f.write(f"Significant differences: {significant_count}\n")
        f.write(f"Non-significant differences: {total_comparisons - significant_count}\n\n")
        
        f.write("DETAILED INTERPRETATIONS:\n\n")
        ref_name = results_summary[0]['model_name']
        ref_auc = results_summary[0]['auc']
        
        for i, result in enumerate(results_summary[1:], 1):  # åŸºæº–ãƒ¢ãƒ‡ãƒ«(index=0)ã‚’ã‚¹ã‚­ãƒƒãƒ—
            f.write(f"{i}. {ref_name} vs {result['model_name']}:\n")
            f.write(f"   AUC Difference: {result['auc'] - ref_auc:+.4f}\n")
            f.write(f"   P-value: {result['p_value']:.6f}\n")
            
            if result['significant']:
                f.write("   âœ“ SIGNIFICANT DIFFERENCE\n")
                if result['auc'] > ref_auc:
                    f.write(f"   â†’ {result['model_name']} performs significantly better\n")
                else:
                    f.write(f"   â†’ {ref_name} performs significantly better\n")
            else:
                f.write("   âœ— No significant difference\n")
            f.write("\n")

def extract_scores_for_csv(pred):
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ã‚¹ã‚³ã‚¢æŠ½å‡ºï¼ˆæ—¢ã«æ­£è¦åŒ–æ¸ˆã¿ã®ç¢ºç‡ç”¨ï¼‰
    predict.pyã¨åŒã˜æ–¹æ³•ã§AUCã‚’è¨ˆç®—
    """
    print(f"  Prediction array shape: {pred.shape}")
    print(f"  First few prediction values: {pred[:3]}")
    
    if pred.ndim > 1 and pred.shape[1] == 2:
        # 2åˆ—ã®å ´åˆï¼šprob_class_0, prob_class_1ï¼ˆæ—¢ã«æ­£è¦åŒ–æ¸ˆã¿ï¼‰
        row_sums = np.sum(pred, axis=1)
        print(f"  Row sums (first 5): {row_sums[:5]}")
        
        # ç¢ºç‡ãŒæ­£è¦åŒ–æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆåˆè¨ˆãŒç´„1.0ï¼‰
        if np.allclose(row_sums, 1.0, atol=0.01):
            print("  âœ“ Confirmed: These are normalized probabilities")
            print("  â†’ Using prob_class_1 directly (same as predict.py)")
            # predict.pyã¨åŒã˜ï¼šæ­£è¦åŒ–æ¸ˆã¿ç¢ºç‡ã®ã‚¯ãƒ©ã‚¹1ã‚’ãã®ã¾ã¾ä½¿ç”¨
            return pred[:, 1]
        else:
            print("  âš ï¸  Values don't sum to 1 - these might be logits")
            print("  â†’ Applying softmax to convert to probabilities")
            # Logitsã®å ´åˆã¯Softmaxã‚’é©ç”¨
            softmax_probs = softmax(pred)
            return softmax_probs[:, 1]
    elif pred.ndim == 1:
        print("  Single column - using as-is")
        return pred
    else:
        print("  Multiple columns - applying softmax")
        return softmax(pred)[:, 1]

def run_multi_analysis():
    """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ä¸€æ‹¬æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œ"""
    print("=== Multi-Model Hanley McNeil AUC Comparison Test ===\n")
    print("Using predefined paths from DEFAULT_PATHS configuration...")
    
    # è¨­å®šè¡¨ç¤º
    print("\nğŸ“ Configuration:")
    print(f"  Reference: {DEFAULT_PATHS['reference']} ({DEFAULT_PATHS['reference_name']})")
    print(f"  Comparison models: {len(DEFAULT_PATHS['models'])}")
    for i, model in enumerate(DEFAULT_PATHS['models']):
        print(f"    {i+1}. {model['path']} ({model['name']})")
    print()
    
    # åŸºæº–ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    print("Loading reference model...")
    try:
        ref_pred, ref_true, _ = load_prediction_results(DEFAULT_PATHS['reference'])
        print(f"âœ“ {DEFAULT_PATHS['reference_name']} loaded successfully")
    except Exception as e:
        print(f"âœ— Error loading reference model: {e}")
        print(f"Please check the file format and content structure.")
        return
    
    # åŸºæº–ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚³ã‚¢æŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰
    ref_scores = extract_scores_for_csv(ref_pred)
    ref_auc = roc_auc_score(ref_true, ref_scores)
    
    print(f"\nğŸ¯ Reference Model Summary:")
    print(f"  {DEFAULT_PATHS['reference_name']}: AUC = {ref_auc:.6f}, n = {len(ref_true)}")
    print(f"  Positive: {np.sum(ref_true == 1)} ({100*np.sum(ref_true == 1)/len(ref_true):.1f}%)")
    print(f"  Score range: [{np.min(ref_scores):.6f}, {np.max(ref_scores):.6f}]")
    
    # predict.pyã¨åŒã˜AUCå€¤ã«ãªã£ã¦ã„ã‚‹ã‹æ¤œè¨¼ç”¨ã®æƒ…å ±
    print(f"\nğŸ“Š AUC Calculation Verification:")
    print(f"  Using scores (class 1 probabilities): {ref_scores[:5]} ...")
    print(f"  True labels: {ref_true[:5]} ...")
    print(f"  â†’ This should match predict.py AUC value: {ref_auc:.6f}")
    
    # æ¯”è¼ƒãƒ¢ãƒ‡ãƒ«ã®å‡¦ç†
    all_results = []
    comparison_data = []
    results_summary = []
    
    # åŸºæº–ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒªãƒ¼ã«è¿½åŠ 
    results_summary.append({
        'model_name': DEFAULT_PATHS['reference_name'],
        'auc': ref_auc,
        'p_value': None,
        'significant': False,
        'ci_lower': None,
        'ci_upper': None,
        'sample_size': len(ref_true)
    })
    
    print(f"\nğŸ”¬ Performing comparisons...")
    
    for i, model_config in enumerate(DEFAULT_PATHS['models']):
        model_path = model_config['path']
        model_name = model_config['name']
        
        print(f"\n--- Comparison {i+1}: {DEFAULT_PATHS['reference_name']} vs {model_name} ---")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            comp_pred, comp_true, _ = load_prediction_results(model_path)
            comp_scores = extract_scores_for_csv(comp_pred)  # ä¿®æ­£ç‰ˆã‚’ä½¿ç”¨
            comp_auc = roc_auc_score(comp_true, comp_scores)
            
            print(f"  {model_name}: AUC = {comp_auc:.6f}, n = {len(comp_true)}")
            print(f"  Score range: [{np.min(comp_scores):.6f}, {np.max(comp_scores):.6f}]")
            
            # Hanley McNeilæ¤œå®šå®Ÿè¡Œ
            results = hanley_mcneil_independent_test(
                ref_true, ref_scores, comp_true, comp_scores, DEFAULT_PATHS['alpha']
            )
            
            # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ¤œå®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            bootstrap_results = {}
            if DEFAULT_PATHS['bootstrap']:
                print("    Running bootstrap test...")
                bootstrap_results = bootstrap_auc_comparison_independent(
                    ref_true, ref_scores, comp_true, comp_scores, alpha=DEFAULT_PATHS['alpha']
                )
            
            # çµæœä¿å­˜
            comparison_result = {
                'comparison_id': i + 1,
                'reference_name': DEFAULT_PATHS['reference_name'],
                'comparison_name': model_name,
                'hanley_mcneil': results,
                'bootstrap': bootstrap_results
            }
            all_results.append(comparison_result)
            
            # ãƒ—ãƒ­ãƒƒãƒˆç”¨ãƒ‡ãƒ¼ã‚¿
            comparison_data.append((comp_true, comp_scores, model_name))
            
            # ã‚µãƒãƒªãƒ¼ç”¨ãƒ‡ãƒ¼ã‚¿
            results_summary.append({
                'model_name': model_name,
                'auc': comp_auc,
                'p_value': results['p_value'],
                'significant': results['significant'],
                'ci_lower': results['ci_lower'],
                'ci_upper': results['ci_upper'],
                'sample_size': len(comp_true)
            })
            
            # çµæœè¡¨ç¤º
            print(f"    AUC difference: {comp_auc - ref_auc:+.6f}")
            print(f"    P-value: {results['p_value']:.6f}")
            print(f"    Significant: {'YES âœ“' if results['significant'] else 'NO âœ—'}")
            
        except Exception as e:
            print(f"  âœ— Error processing {model_name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\n" + "="*80)
    print("=== FINAL RESULTS SUMMARY ===")
    print("="*80)
    
    print(f"{'Model':<25} {'AUC':<12} {'P-value':<12} {'Significant':<12}")
    print("-" * 65)
    
    for result in results_summary:
        pval_str = f"{result['p_value']:.6f}" if result['p_value'] is not None else "N/A"
        print(f"{result['model_name']:<25} {result['auc']:<12.6f} "
              f"{pval_str:<12} {str(result['significant']):<12}")
    
    # çµ±è¨ˆçš„ã‚µãƒãƒªãƒ¼
    significant_count = sum([r['significant'] for r in results_summary[1:]])
    total_comparisons = len(results_summary) - 1
    
    print(f"\nğŸ“Š Statistical Summary:")
    print(f"  Total comparisons: {total_comparisons}")
    print(f"  Significant differences: {significant_count}")
    print(f"  Non-significant differences: {total_comparisons - significant_count}")
    
    # çµæœä¿å­˜
    print(f"\nğŸ’¾ Saving results to: {DEFAULT_PATHS['output']}")
    output_dir = Path(DEFAULT_PATHS['output'])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_base = output_dir / "multi_comparison"
    save_multi_results(all_results, results_summary, str(output_base))
    
    # ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
    if comparison_data:
        detailed_plot_path = output_base.with_name("detailed_comparisons.png")
        create_multi_comparison_plot(
            (ref_true, ref_scores, DEFAULT_PATHS['reference_name']),
            comparison_data,
            str(detailed_plot_path)
        )
        
        summary_plot_path = output_base.with_name("summary_plot.png")
        create_summary_plot(results_summary, str(summary_plot_path))
        
        print(f"\nğŸ“„ Output Files:")
        print(f"  Detailed results (JSON): {output_base}_detailed_results.json")
        print(f"  Summary (CSV): {output_base}_summary.csv")
        print(f"  Report (TXT): {output_base}_multi_comparison_report.txt")
        print(f"  Detailed plots: {detailed_plot_path}")
        print(f"  Summary plot: {summary_plot_path}")
    
    print(f"\nğŸ‰ Multi-model analysis completed successfully!")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    run_multi_analysis()

if __name__ == '__main__':
    main()